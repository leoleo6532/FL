{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leo6532/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/leo6532/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/leo6532/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/leo6532/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/leo6532/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/leo6532/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/leo6532/.local/lib/python3.6/site-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.26.2) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n",
      "/home/leo6532/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/leo6532/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/leo6532/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/leo6532/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/leo6532/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/leo6532/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import syft as sy\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets ,models,transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.optim import lr_scheduler\n",
    "from pathlib import Path\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available!  Training on GPU ...\n"
     ]
    }
   ],
   "source": [
    "#Determine if there is a GPU\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if not train_on_gpu:\n",
    "    print('CUDA is not available.  Training on CPU ...')\n",
    "else:\n",
    "    print('CUDA is available!  Training on GPU ...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_train=\"./train\"\n",
    "PATH_val=\"./val\"\n",
    "PATH_test=\"./test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "val\n",
      "test\n"
     ]
    }
   ],
   "source": [
    "TRAIN =Path(PATH_train)\n",
    "VALID = Path(PATH_val)\n",
    "TEST=Path(PATH_test)\n",
    "print(TRAIN)\n",
    "print(VALID)\n",
    "print(TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of subprocesses to use for data loading\n",
    "num_workers = 0\n",
    "# how many samples per batch to load\n",
    "batch_size = 32\n",
    "# learning rate\n",
    "LR = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert data to a normalized torch.FloatTensor\n",
    "train_transforms = transforms.Compose([transforms.Resize((50,50)),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                           [0.229, 0.224, 0.225])])\n",
    "\n",
    "valid_transforms = transforms.Compose([transforms.Resize((50,50)),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                           [0.229, 0.224, 0.225])])\n",
    "\n",
    "test_transforms = transforms.Compose([transforms.Resize((50,50)),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                           [0.229, 0.224, 0.225])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the training and test datasets\n",
    "train_data = datasets.ImageFolder(TRAIN, transform=train_transforms)\n",
    "valid_data = datasets.ImageFolder(VALID,transform=valid_transforms)\n",
    "test_data = datasets.ImageFolder(TEST, transform=test_transforms)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 切割資料方法1\n",
    "## 用random_split 去切割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1_size = int(0.8*len(train_data))\n",
    "dataset2_size = len(train_data) - dataset1_size\n",
    "dataset1, dataset2 = torch.utils.data.random_split(train_data, [dataset1_size, dataset2_size])\n",
    "\n",
    "bob_train_loader = torch.utils.data.DataLoader(dataset1, batch_size=batch_size, num_workers=num_workers,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sick': 0, 'unsick': 1}\n",
      "{'sick': 0, 'unsick': 1}\n",
      "{'sick': 0, 'unsick': 1}\n"
     ]
    }
   ],
   "source": [
    "print(train_data.class_to_idx)\n",
    "print(valid_data.class_to_idx)\n",
    "print(test_data.class_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data loaders (combine dataset and sampler)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, num_workers=num_workers,shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_data, batch_size=batch_size,  num_workers=num_workers,shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size,  num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 3, 50, 50]), torch.Size([32]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images,labels=next(iter(train_loader))\n",
    "images.shape,labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CNN Model\n",
    "class CNN_Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_Model, self).__init__()\n",
    "        self.cnn1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=0) #output_shape=(16,220,220) 48\n",
    "        #(224-5+1)/1 #(weigh-kernel+1)/stride 無條件進位\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=2) #output_shape=(16,110,110) #(220/2) 16 24 24\n",
    "        self.cnn2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=0) #output_shape=(32,106,106) 20 20\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=1) #output_shape=(32,53,53) 10 10\n",
    "        self.cnn3 = nn.Conv2d(in_channels=32, out_channels=16, kernel_size=1, stride=1, padding=0) #output_shape=(16,51,51) 10 10\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.maxpool3 = nn.MaxPool2d(kernel_size=1) #output_shape=(16,25,25) 5 5\n",
    "        self.cnn4 = nn.Conv2d(in_channels=16, out_channels=8, kernel_size=1, stride=1, padding=0) #output_shape=(8,23,23) 5 5\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.maxpool4 = nn.MaxPool2d(kernel_size=2) #output_shape=(8,11,11) 8 5 5\n",
    "        # Fully connected 1 ,#input_shape=(8*12*12)\n",
    "        self.fc1 = nn.Linear(8 * 11 * 11, 512) \n",
    "        #self.relu5 = nn.ReLU() # activation\n",
    "        self.fc2 = nn.Linear(512, 2) \n",
    "        #self.output = nn.Softmax(dim=1)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.cnn1(x) # Convolution 1\n",
    "        out = self.relu1(out)\n",
    "        out = self.maxpool1(out)# Max pool 1\n",
    "        out = self.cnn2(out) # Convolution 2\n",
    "        out = self.relu2(out) \n",
    "        out = self.maxpool2(out) # Max pool 2\n",
    "        out = self.cnn3(out) # Convolution 3\n",
    "        out = self.relu3(out)\n",
    "        out = self.maxpool3(out) # Max pool 3\n",
    "        out = self.cnn4(out) # Convolution 4\n",
    "        out = self.relu4(out)\n",
    "        out = self.maxpool4(out) # Max pool 4\n",
    "#         print(out.size())\n",
    "        out = out.view(-1, 8*11*11) # last CNN faltten con. Linear NN\n",
    "#     out = out.view(out.size(0), -1)\n",
    "        out = self.fc1(out) # Linear function (readout)\n",
    "        out = self.fc2(out)\n",
    "        #out = self.output(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN_Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 16, 48, 48]             448\n",
      "              ReLU-2           [-1, 16, 48, 48]               0\n",
      "         MaxPool2d-3           [-1, 16, 24, 24]               0\n",
      "            Conv2d-4           [-1, 32, 22, 22]           4,640\n",
      "              ReLU-5           [-1, 32, 22, 22]               0\n",
      "         MaxPool2d-6           [-1, 32, 22, 22]               0\n",
      "            Conv2d-7           [-1, 16, 22, 22]             528\n",
      "              ReLU-8           [-1, 16, 22, 22]               0\n",
      "         MaxPool2d-9           [-1, 16, 22, 22]               0\n",
      "           Conv2d-10            [-1, 8, 22, 22]             136\n",
      "             ReLU-11            [-1, 8, 22, 22]               0\n",
      "        MaxPool2d-12            [-1, 8, 11, 11]               0\n",
      "           Linear-13                  [-1, 512]         496,128\n",
      "           Linear-14                    [-1, 2]           1,026\n",
      "================================================================\n",
      "Total params: 502,906\n",
      "Trainable params: 502,906\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.03\n",
      "Forward/backward pass size (MB): 1.23\n",
      "Params size (MB): 1.92\n",
      "Estimated Total Size (MB): 3.18\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(model.cuda(), (3, 50, 50))\n",
    "#summary(model, (3, 50, 50),device = \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from tqdm import tqdm_notebook as tqdm\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim    \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR) \n",
    "criterion = nn.CrossEntropyLoss() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#TEST MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(loaders, model, criterion, use_cuda):\n",
    "\n",
    "    # monitor test loss and accuracy\n",
    "    test_loss = 0.\n",
    "    correct = 0.\n",
    "    total = 0.\n",
    "\n",
    "    model.eval()\n",
    "    for batch_idx, (data, target) in enumerate(loaders):\n",
    "        # move to GPU\n",
    "        if use_cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "        # update average test loss \n",
    "        test_loss = test_loss + ((1 / (batch_idx + 1)) * (loss.data - test_loss))\n",
    "        # convert output probabilities to predicted class\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        # compare predictions to true label\n",
    "        correct += np.sum(np.squeeze(pred.eq(target.data.view_as(pred))).cpu().numpy())\n",
    "        total += data.size(0)\n",
    "            \n",
    "    print('Test Loss: {:.6f}'.format(test_loss))\n",
    "\n",
    "    print('Test Accuracy: %2d%% (%2d/%2d)' % (\n",
    "        100. * correct / total, correct, total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.666572\n",
      "Test Accuracy: 82% (1340/1622)\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "model.cuda()\n",
    "test(test_loader, model, criterion, use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(n_epochs, train_loader, valid_loader, model, optimizer, criterion, worker):\n",
    "    train_on_gpu = torch.cuda.is_available()\n",
    "    train_loss_i = []\n",
    "    valid_loss_i = []\n",
    "\n",
    "\n",
    "#     n_epochs = 10 #170\n",
    "\n",
    "\n",
    "    valid_loss_min = np.Inf # track change in validation loss\n",
    "    model.send(worker)\n",
    "    \n",
    "    #train_losses,valid_losses=[],[]\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "\n",
    "        # keep track of training and validation loss\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        print('running epoch: {}'.format(epoch))\n",
    "        train_dataset_size = 0\n",
    "        valid_dataset_size = 0\n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        model.train()\n",
    "        \n",
    "        for data, target in tqdm(train_loader):\n",
    "#             print(data.location)\n",
    "            if(data.location != worker):\n",
    "                break\n",
    "                \n",
    "            \n",
    "            train_dataset_size += len(data)\n",
    "\n",
    "            # move tensors to GPU if CUDA is available\n",
    "            if train_on_gpu:\n",
    "                data, target = data.cuda(),  target.cuda()\n",
    "            # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "            # calculate the batch loss\n",
    "            loss = criterion(output, target)\n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "            # update training loss\n",
    "            train_loss += loss.get().item()*len(data)\n",
    "            \n",
    "\n",
    "        for data, target in tqdm(valid_loader):\n",
    "            \n",
    "            valid_dataset_size += len(data)\n",
    "            \n",
    "            # move tensors to GPU if CUDA is available\n",
    "            if train_on_gpu:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output = model(data)\n",
    "            # calculate the batch loss\n",
    "            loss = criterion(output, target)\n",
    "            # update average validation loss \n",
    "            valid_loss += loss.get().item()* len(data)\n",
    "\n",
    "        \n",
    "        train_loss = train_loss/train_dataset_size\n",
    "        valid_loss = valid_loss/valid_dataset_size\n",
    "\n",
    "        # print training/validation statistics \n",
    "        print('\\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "             train_loss,  valid_loss))    \n",
    "        #作圖接收矩陣\n",
    "        train_loss_i.append(train_loss)\n",
    "        valid_loss_i.append(valid_loss)\n",
    "        if valid_loss <= valid_loss_min:\n",
    "            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "            valid_loss_min,\n",
    "            valid_loss))\n",
    "            valid_loss_min = valid_loss\n",
    "            \n",
    "    #作圖區\n",
    "    print(train_loss_i) \n",
    "    print(valid_loss_i)    \n",
    "\n",
    "    #圖區    \n",
    "    plt.plot(range(n_epochs), train_loss_i,'b-', label='Training_loss')\n",
    "    plt.plot(range(n_epochs), valid_loss_i,'g-', label='validation_loss')\n",
    "    plt.title('Training & Validation loss')\n",
    "    plt.xlabel('Number of epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 虛擬環境執行 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Torch was already hooked... skipping hooking process\n"
     ]
    }
   ],
   "source": [
    "hook = sy.TorchHook(torch)\n",
    "bob = sy.VirtualWorker(hook, id=\"bob\")\n",
    "alice = sy.VirtualWorker(hook, id=\"alice\")\n",
    "secure_worker = sy.VirtualWorker(hook, id=\"secure_worker\")\n",
    "compute_nodes = [ben, alice]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 切割資料方法2\n",
    "## for loop去切割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'ScriptModule' has no attribute 'get_msgpack_code'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-2ef8f1807217>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbob_train_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mben\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mben\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mbob_train_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/syft/generic/abstract/hookable.py\u001b[0m in \u001b[0;36mhooked_method\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhooked_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mmap_chain_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"_before_{method_name}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mreturn_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhookable_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0mreturn_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreduce_chain_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"_after_{method_name}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mreturn_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/syft/frameworks/torch/tensors/interpreters/native.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, inplace, user, local_autograd, requires_grad, preinitialize_grad, no_wrap, garbage_collect_data, *location)\u001b[0m\n\u001b[1;32m    488\u001b[0m                 \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m                 \u001b[0mpreinitialize_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreinitialize_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 490\u001b[0;31m                 \u001b[0mgarbage_collect_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgarbage_collect_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    491\u001b[0m             )\n\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/syft/workers/base.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, obj, workers, ptr_id, garbage_collect_data, requires_grad, create_pointer, **kwargs)\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m         \u001b[0;31m# Send the object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 431\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    432\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/syft/workers/base.py\u001b[0m in \u001b[0;36msend_obj\u001b[0;34m(self, obj, location)\u001b[0m\n\u001b[1;32m    587\u001b[0m                 \u001b[0mreceive\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m         \"\"\"\n\u001b[0;32m--> 589\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mObjectMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m     def request_obj(\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/syft/workers/base.py\u001b[0m in \u001b[0;36msend_msg\u001b[0;34m(self, message, location)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;31m# Step 1: serialize the message to a binary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m         \u001b[0mbin_message\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserde\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0;31m# Step 2: send the message and wait for a response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/syft/serde/serde.py\u001b[0m in \u001b[0;36mserialize\u001b[0;34m(obj, worker, simplified, force_full_simplification, strategy)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmsgpack_serialize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mstrategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimplified\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_full_simplification\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/syft/serde/msgpack/serde.py\u001b[0m in \u001b[0;36mserialize\u001b[0;34m(obj, worker, simplified, force_full_simplification)\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0mworker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msyft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal_worker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m     \u001b[0msimple_objects\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_serialize_msgpack_simple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimplified\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_full_simplification\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_serialize_msgpack_binary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimple_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/syft/serde/msgpack/serde.py\u001b[0m in \u001b[0;36m_serialize_msgpack_simple\u001b[0;34m(obj, worker, simplified, force_full_simplification)\u001b[0m\n\u001b[1;32m    280\u001b[0m             \u001b[0msimple_objects\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_force_full_simplify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m             \u001b[0msimple_objects\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_simplify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0msimple_objects\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/syft/serde/msgpack/serde.py\u001b[0m in \u001b[0;36m_simplify\u001b[0;34m(worker, obj, **kwargs)\u001b[0m\n\u001b[1;32m    471\u001b[0m         \u001b[0;31m# the framework\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m         \u001b[0mmsgpack_global_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstale_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mcurrent_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmsgpack_global_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimplifiers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    474\u001b[0m             result = (\n\u001b[1;32m    475\u001b[0m                 \u001b[0mmsgpack_global_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimplifiers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcurrent_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/syft/serde/msgpack/serde.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m             \u001b[0mself\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/syft/serde/msgpack/serde.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;31m# Register native and torch types\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcurr_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msimplifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetailer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_MAP_TO_SIMPLFIERS_AND_DETAILERS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m             \u001b[0m_add_simplifier_and_detailer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurr_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimplifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetailer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0;31m# # Register syft objects with custom simplify and detail methods\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/syft/serde/msgpack/serde.py\u001b[0m in \u001b[0;36m_add_simplifier_and_detailer\u001b[0;34m(curr_type, simplifier, detailer, forced)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_add_simplifier_and_detailer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurr_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimplifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetailer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforced\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m             \u001b[0mtype_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproto_type_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurr_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mforced\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forced_full_simplifiers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcurr_type\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtype_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforced_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimplifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/syft/serde/msgpack/proto.py\u001b[0m in \u001b[0;36mproto_type_info\u001b[0;34m(cls)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtype_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mproto_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"TYPES\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mTypeInfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtype_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproto_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"TYPES\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0;32melif\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_msgpack_code\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__qualname__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mTypeInfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtype_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_msgpack_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'ScriptModule' has no attribute 'get_msgpack_code'"
     ]
    }
   ],
   "source": [
    "bob_train_dataset = []\n",
    "for batch_idx, (data, target) in enumerate(train_loader):\n",
    "    data = data.send(bob)\n",
    "    target = target.send(bob)\n",
    "    bob_train_dataset.append((b_data, b_target))\n",
    "\n",
    "bob_valid_dataset = []\n",
    "for batch_idx, (data, target) in enumerate(valid_loader):\n",
    "    data = data.send(bob)\n",
    "    target = target.send(bob)\n",
    "    bob_valid_dataset.append((data, target))\n",
    "\n",
    "alice_train_dataset = []\n",
    "for batch_idx, (data, target) in enumerate(train_loader):\n",
    "    data = data.send(alice)\n",
    "    target = target.send(alice)\n",
    "    alice_train_dataset.append((data, target))\n",
    "\n",
    "alice_valid_dataset = []\n",
    "for batch_idx, (data, target) in enumerate(valid_loader):\n",
    "    data = data.send(alice)\n",
    "    target = target.send(alice)\n",
    "    alice_valid_dataset.append((data, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'DataLoader' object does not support indexing",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-6247d2100dcd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'DataLoader' object does not support indexing"
     ]
    }
   ],
   "source": [
    "data, target = \n",
    "\n",
    "data = data.send(bob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 切割資料方法3\n",
    "## sy.FederatedDataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "federated_train_loader = sy.FederatedDataLoader(train_data.federate((bob, alice)), batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get一次就好 第2次會error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "bobs_model = CNN_Model().cuda()\n",
    "bob_optimizer = torch.optim.Adam(bobs_model.parameters(), lr=LR) \n",
    "bob_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "alice_model = CNN_Model().cuda()\n",
    "alice_optimizer = torch.optim.Adam(alice_model.parameters(), lr=LR)\n",
    "alice_criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "123\n",
      "running epoch: 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "938cb72fffaa46349874d07bb882fba4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=163.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "880edb1a0ee04b5cae059b065ab96877",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=41.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tTraining Loss: 0.043191 \tValidation Loss: 0.964359\n",
      "Validation loss decreased (inf --> 0.964359).  Saving model ...\n",
      "running epoch: 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "073ddd86733149bf87acaf34c82f5619",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=163.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d09d6c7cf39400ca7916eb2f372883d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=41.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tTraining Loss: 0.044623 \tValidation Loss: 0.778664\n",
      "Validation loss decreased (0.964359 --> 0.778664).  Saving model ...\n",
      "running epoch: 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d13a2bcf14d48728e8b0d8cf8d39acf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=163.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2357785d2c3495c81f634379a27594c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=41.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tTraining Loss: 0.021849 \tValidation Loss: 1.132103\n",
      "running epoch: 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1d4d48deaf44d1a8633b9ba42231d81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=163.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb1e317862304a3f89ed339eb504d687",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=41.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tTraining Loss: 0.015983 \tValidation Loss: 1.304753\n",
      "running epoch: 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c83bb1447ed4ada91fe44ecb529d18b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=163.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b6dab2b377844d3acbba97b43cdad30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=41.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tTraining Loss: 0.030900 \tValidation Loss: 1.140878\n",
      "running epoch: 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef64b3a874d5416d98d12269058fd640",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=163.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b18c1ea6d96c4732a7d2effe9de8f749",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=41.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tTraining Loss: 0.033200 \tValidation Loss: 1.209200\n",
      "running epoch: 7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71a8d832af0042da9accf49cd5e16909",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=163.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d5f6e9c3851426fb579cbef8564216b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=41.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tTraining Loss: 0.031240 \tValidation Loss: 0.959422\n",
      "running epoch: 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7544369567ec4e6081eb902fcc94e0e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=163.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9943a7d3be374fe8b854eba19cd97c3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=41.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tTraining Loss: 0.019317 \tValidation Loss: 1.414001\n",
      "running epoch: 9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b11bc2a7794a40bcb332d584be2283b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=163.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7a90ee110f64534ad80e367b430cc07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=41.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tTraining Loss: 0.030750 \tValidation Loss: 1.214920\n",
      "running epoch: 10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b22c706437da45fe9b48ad45e88580ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=163.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4f6d468f354452084067ed15b78b8af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=41.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tTraining Loss: 0.007923 \tValidation Loss: 1.563713\n",
      "[0.043191183175291206, 0.044622848414031184, 0.02184895850202373, 0.015983445275052792, 0.03089989524988947, 0.03320000523635595, 0.031239960196851383, 0.019317055601179196, 0.03075013527618292, 0.007923267872227337]\n",
      "[0.964358597608971, 0.7786636776653454, 1.1321032099144313, 1.3047532493517136, 1.1408776413985373, 1.2092003093266956, 0.9594216997129879, 1.4140012690491555, 1.2149200176356292, 1.5637129456632028]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "update(10, bob_train_dataset, bob_valid_dataset, bobs_model, bob_optimizer, bob_criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update(5, alice_train_dataset, alice_valid_dataset, alice_model, alice_optimizer, alice_criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alice_model.move(secure_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bobs_model.move(secure_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_param = list(map(lamba x,y : (x+y)/2, list(bobs_model.parameters()), list(alice_model.parameters())))\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.parameters.set_(agg_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(test_loader, model, criterion, use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.202216\n",
      "Test Accuracy: 87% (1424/1622)\n"
     ]
    }
   ],
   "source": [
    "test(test_loader, bobs_model, criterion, use_cuda)\n",
    "# test(test_loader, alices_model, criterion, use_cuda)\n",
    "# test(test_loader, new_model, criterion, use_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Node gradient send to server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "bobS_gradient = bobs_model.copy().send(server)\n",
    "alices_gardient = alices_model.copy().send(server)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN_Model2(\n",
       "  (cnn1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (relu1): ReLU()\n",
       "  (maxpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (cnn2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (relu2): ReLU()\n",
       "  (maxpool2): MaxPool2d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "  (cnn3): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (relu3): ReLU()\n",
       "  (maxpool3): MaxPool2d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "  (cnn4): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (relu4): ReLU()\n",
       "  (maxpool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=968, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bobS_gradient.get()\n",
    "alices_gardient.get()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use update gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[[[ 4.3631e-02, -1.4934e-01,  3.0487e-02],\n",
      "          [ 1.3620e-01, -8.7945e-04,  1.4578e-01],\n",
      "          [ 4.6119e-02,  1.5437e-01, -1.4302e-01]],\n",
      "\n",
      "         [[-9.7464e-02,  1.6773e-01,  1.8670e-01],\n",
      "          [ 1.8652e-01,  2.2554e-02, -8.8016e-02],\n",
      "          [ 1.5032e-01,  7.0171e-02,  7.0772e-02]],\n",
      "\n",
      "         [[ 1.4363e-01, -4.8580e-02,  5.8282e-02],\n",
      "          [ 1.6558e-01,  1.0677e-01,  8.5537e-02],\n",
      "          [-1.6444e-01, -7.3739e-02,  2.0141e-01]]],\n",
      "\n",
      "\n",
      "        [[[-8.5975e-02, -1.3932e-02,  1.6522e-01],\n",
      "          [ 7.0449e-02, -1.3199e-02,  2.0559e-01],\n",
      "          [-9.9543e-02,  1.3346e-01, -7.1039e-02]],\n",
      "\n",
      "         [[ 1.2586e-01,  5.6154e-02,  8.0827e-03],\n",
      "          [-1.4597e-01,  6.6565e-02,  2.8507e-02],\n",
      "          [ 5.0013e-02, -1.7375e-01, -6.9327e-02]],\n",
      "\n",
      "         [[-8.4501e-02, -4.9251e-02,  5.4388e-02],\n",
      "          [ 9.6579e-02,  9.4361e-02,  1.1827e-02],\n",
      "          [-1.0936e-01, -1.6782e-01,  7.0798e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 1.8551e-01,  1.3399e-01, -9.2045e-03],\n",
      "          [-6.9006e-02, -9.9343e-02, -2.2587e-02],\n",
      "          [ 1.0074e-01,  8.1403e-02, -1.2713e-01]],\n",
      "\n",
      "         [[ 5.4202e-02, -1.7183e-01,  7.6309e-02],\n",
      "          [-1.7683e-01, -1.3037e-01,  1.5838e-01],\n",
      "          [ 5.0472e-02,  3.7817e-02,  8.8251e-02]],\n",
      "\n",
      "         [[ 1.3319e-01, -1.4206e-01,  3.7994e-02],\n",
      "          [-7.0965e-02, -1.6866e-01, -9.9768e-03],\n",
      "          [ 5.3767e-02, -7.7888e-03, -3.4084e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 1.6546e-01,  1.8890e-01,  1.8107e-01],\n",
      "          [ 1.2737e-01,  9.2078e-02,  3.2086e-02],\n",
      "          [-1.3546e-01,  1.8214e-02,  8.8737e-02]],\n",
      "\n",
      "         [[ 1.1818e-01, -1.5643e-03, -1.6075e-01],\n",
      "          [-1.7785e-01,  9.1807e-02, -1.1615e-02],\n",
      "          [ 7.3860e-03, -1.3057e-01,  1.4610e-02]],\n",
      "\n",
      "         [[-1.1603e-01,  1.8269e-01,  6.2199e-03],\n",
      "          [-1.5365e-01, -9.4821e-02, -1.7314e-01],\n",
      "          [-1.6089e-01, -1.2426e-01,  3.2254e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 1.7162e-01,  6.9262e-02, -9.8486e-02],\n",
      "          [-3.8549e-02,  7.0763e-02,  8.3920e-02],\n",
      "          [ 1.6748e-01, -4.6721e-02,  1.3206e-01]],\n",
      "\n",
      "         [[-9.8297e-02,  4.0933e-02, -3.5446e-02],\n",
      "          [ 1.9894e-01, -8.0106e-02,  1.4752e-01],\n",
      "          [ 1.7521e-01, -1.0857e-01,  3.4780e-02]],\n",
      "\n",
      "         [[ 1.1442e-01,  7.5967e-02,  7.9308e-02],\n",
      "          [ 2.1102e-01, -2.9411e-02,  4.7760e-02],\n",
      "          [ 4.0335e-02,  1.6888e-01,  1.0813e-03]]],\n",
      "\n",
      "\n",
      "        [[[-1.7353e-02,  1.0253e-01, -2.1410e-02],\n",
      "          [-1.2883e-01,  1.0809e-01, -4.9637e-02],\n",
      "          [ 2.2536e-01, -1.4153e-01, -7.8478e-02]],\n",
      "\n",
      "         [[-1.0695e-01, -1.1107e-01,  1.3096e-01],\n",
      "          [-3.5647e-02, -7.5420e-02, -4.4011e-02],\n",
      "          [ 1.9704e-01,  6.5398e-02,  3.3914e-02]],\n",
      "\n",
      "         [[ 1.4895e-01,  3.6371e-03, -6.3869e-02],\n",
      "          [-4.6107e-02, -7.5508e-04,  4.9650e-02],\n",
      "          [-1.2972e-01, -9.9739e-02,  1.2699e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 1.1587e-01, -1.6164e-01, -1.3823e-01],\n",
      "          [-1.0077e-01, -1.6175e-01, -1.5205e-02],\n",
      "          [-6.2315e-02, -1.6245e-01,  9.2916e-02]],\n",
      "\n",
      "         [[ 1.5893e-01, -1.7664e-01, -1.6197e-01],\n",
      "          [-1.0514e-01, -5.0030e-02, -9.2370e-02],\n",
      "          [ 1.7724e-01,  1.1469e-01,  2.7581e-02]],\n",
      "\n",
      "         [[ 1.3152e-01,  1.9197e-01,  1.1506e-01],\n",
      "          [ 2.9656e-02,  1.2842e-01, -6.7508e-02],\n",
      "          [ 3.3763e-02,  9.9216e-02, -7.5637e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 1.9016e-01, -4.6861e-02,  5.7697e-03],\n",
      "          [ 1.1225e-01, -9.1559e-02, -1.8410e-01],\n",
      "          [-6.0307e-02, -1.4679e-02,  7.4634e-02]],\n",
      "\n",
      "         [[ 9.9000e-02,  1.0819e-01,  1.3855e-02],\n",
      "          [ 6.8785e-02, -1.8631e-01, -9.6576e-02],\n",
      "          [ 8.5325e-02, -1.3218e-01, -2.7619e-02]],\n",
      "\n",
      "         [[-9.0379e-02, -1.6815e-01, -8.4654e-02],\n",
      "          [ 2.3180e-02,  1.8979e-01, -3.9711e-02],\n",
      "          [-3.0373e-02,  1.5801e-01,  9.0652e-02]]],\n",
      "\n",
      "\n",
      "        [[[-1.3366e-01, -1.5625e-01, -1.2470e-02],\n",
      "          [ 1.5231e-02,  1.2348e-01,  7.3688e-02],\n",
      "          [ 1.3557e-01,  9.0658e-03, -9.9181e-02]],\n",
      "\n",
      "         [[-9.2715e-02,  7.7993e-02, -9.4451e-02],\n",
      "          [ 3.0568e-02,  1.8991e-01, -5.0189e-02],\n",
      "          [-1.6070e-01, -1.7094e-02, -1.3363e-01]],\n",
      "\n",
      "         [[-2.0338e-02, -1.6327e-02, -1.5333e-02],\n",
      "          [-5.2952e-02,  9.3590e-02,  3.6619e-02],\n",
      "          [ 1.4121e-01,  9.5989e-02, -1.3243e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 1.2092e-01, -6.1599e-02,  1.0233e-04],\n",
      "          [ 1.9227e-01, -1.6777e-01,  1.6890e-01],\n",
      "          [ 8.0408e-02, -1.0253e-01, -1.2220e-01]],\n",
      "\n",
      "         [[ 1.5527e-01, -1.7713e-01,  1.7069e-01],\n",
      "          [ 7.9645e-02, -1.6261e-01,  1.0610e-01],\n",
      "          [ 1.8901e-01, -1.5050e-01,  8.3374e-02]],\n",
      "\n",
      "         [[-3.8739e-02, -1.5797e-01, -1.8418e-01],\n",
      "          [-8.6532e-02,  3.6692e-02, -3.9760e-02],\n",
      "          [-1.8864e-01, -9.1220e-02, -3.2454e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 1.9681e-02, -9.8468e-02,  1.4691e-01],\n",
      "          [-1.7197e-01,  1.4855e-01,  6.4704e-02],\n",
      "          [ 1.5958e-01, -9.0820e-02, -4.6226e-02]],\n",
      "\n",
      "         [[ 1.4132e-01, -1.1181e-01, -1.2138e-01],\n",
      "          [ 7.2466e-02,  1.8797e-01, -1.2530e-01],\n",
      "          [ 1.8679e-02, -8.4718e-02,  1.4787e-01]],\n",
      "\n",
      "         [[ 8.6848e-03, -9.7809e-02,  1.2121e-01],\n",
      "          [-6.8971e-02, -1.2381e-02,  4.0006e-03],\n",
      "          [-1.7600e-01,  4.5635e-02, -1.0917e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 6.6682e-02, -2.5387e-03,  1.6443e-01],\n",
      "          [-1.6799e-01, -1.4031e-01,  1.7479e-01],\n",
      "          [-8.8794e-02, -1.1951e-01,  5.2409e-02]],\n",
      "\n",
      "         [[ 3.9488e-02,  2.1427e-01,  5.7238e-02],\n",
      "          [-6.7219e-02,  8.8692e-02, -1.1126e-01],\n",
      "          [ 1.4525e-01,  3.4796e-02,  1.5084e-02]],\n",
      "\n",
      "         [[-2.6867e-02,  9.9863e-02,  9.6382e-02],\n",
      "          [ 2.2207e-01,  1.2793e-01,  1.0372e-01],\n",
      "          [-1.3643e-01,  3.7219e-02,  1.1549e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 5.3781e-02, -1.7327e-01,  1.3736e-01],\n",
      "          [ 1.0104e-01, -1.6801e-01,  3.4818e-02],\n",
      "          [-1.1490e-01, -5.6930e-02,  3.5104e-02]],\n",
      "\n",
      "         [[ 1.0286e-03,  3.0474e-02,  1.4155e-01],\n",
      "          [ 1.7320e-02,  3.2083e-02,  2.7058e-02],\n",
      "          [-4.4998e-03,  4.5516e-03,  9.4477e-02]],\n",
      "\n",
      "         [[ 1.9391e-01, -1.7886e-01,  8.8307e-02],\n",
      "          [-5.7985e-02, -1.7398e-01,  1.3227e-02],\n",
      "          [ 1.8134e-01, -1.1853e-01,  1.8441e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 1.4025e-01,  7.4543e-02,  4.5672e-02],\n",
      "          [ 2.9480e-02,  1.4517e-02,  5.3703e-02],\n",
      "          [-2.2752e-02,  1.1824e-01,  1.2005e-01]],\n",
      "\n",
      "         [[ 5.0107e-02,  1.5963e-02, -7.7663e-03],\n",
      "          [-1.7536e-01, -1.6142e-01,  1.7884e-01],\n",
      "          [ 2.0036e-02, -1.3797e-01, -2.0505e-01]],\n",
      "\n",
      "         [[ 3.6160e-03,  4.6189e-02, -5.3684e-02],\n",
      "          [-8.8657e-02, -1.1859e-01, -5.8295e-02],\n",
      "          [-4.2987e-03, -2.6285e-02,  1.5389e-01]]],\n",
      "\n",
      "\n",
      "        [[[-7.5312e-02, -6.8243e-02, -1.2560e-02],\n",
      "          [-1.7663e-01,  1.2086e-01,  6.1608e-02],\n",
      "          [-1.4627e-02,  1.3293e-01, -1.0226e-01]],\n",
      "\n",
      "         [[-1.3421e-02,  5.7221e-03,  2.1783e-02],\n",
      "          [-1.7590e-01,  3.4203e-02, -1.2683e-01],\n",
      "          [ 1.3851e-03,  1.0005e-01, -1.1461e-01]],\n",
      "\n",
      "         [[-1.1415e-01,  1.6883e-01, -3.0394e-02],\n",
      "          [ 1.0634e-02, -1.6427e-01,  1.7221e-01],\n",
      "          [ 1.5128e-01, -1.7844e-01, -6.1598e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 9.5063e-02, -1.1687e-01,  1.0895e-01],\n",
      "          [-6.5941e-03, -1.4776e-01,  1.8836e-01],\n",
      "          [ 1.4541e-01,  1.0610e-01,  5.4446e-02]],\n",
      "\n",
      "         [[ 2.6252e-02,  1.9422e-01, -4.1408e-02],\n",
      "          [-3.9225e-02, -6.5568e-02,  2.5424e-02],\n",
      "          [-1.7237e-01, -1.4114e-01, -4.8606e-02]],\n",
      "\n",
      "         [[-6.4112e-02, -1.9138e-02,  9.5031e-02],\n",
      "          [ 4.0902e-02,  3.2487e-02,  4.7678e-02],\n",
      "          [ 1.2673e-01,  1.6542e-01,  9.0737e-02]]]], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.1093,  0.1440,  0.0301, -0.1846,  0.2053, -0.1859,  0.1517, -0.1238,\n",
      "        -0.0458, -0.1293, -0.1437,  0.0682,  0.1259, -0.1342, -0.0795, -0.0661],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([[[[ 0.0165,  0.0158,  0.0653],\n",
      "          [-0.0096,  0.0217, -0.0300],\n",
      "          [-0.0732,  0.0369,  0.0258]],\n",
      "\n",
      "         [[ 0.0221, -0.0383, -0.0708],\n",
      "          [-0.0674,  0.0332, -0.0406],\n",
      "          [-0.0620,  0.0637,  0.0012]],\n",
      "\n",
      "         [[ 0.0702, -0.0870, -0.0355],\n",
      "          [ 0.0038,  0.0341, -0.0819],\n",
      "          [-0.0162, -0.0026,  0.0192]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0307,  0.0729,  0.0521],\n",
      "          [-0.0286,  0.0266, -0.0604],\n",
      "          [-0.0716,  0.0006, -0.0658]],\n",
      "\n",
      "         [[ 0.0463, -0.0565, -0.0345],\n",
      "          [-0.0585,  0.0589,  0.0356],\n",
      "          [-0.0331,  0.0703,  0.0200]],\n",
      "\n",
      "         [[-0.0669, -0.0511, -0.0068],\n",
      "          [-0.0187,  0.0412, -0.0525],\n",
      "          [ 0.0675, -0.0178, -0.0239]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0575,  0.0930,  0.0279],\n",
      "          [ 0.1017,  0.0527, -0.0537],\n",
      "          [ 0.1046,  0.0034,  0.0485]],\n",
      "\n",
      "         [[ 0.0530, -0.0432, -0.0756],\n",
      "          [ 0.0490,  0.0662, -0.0221],\n",
      "          [ 0.0209,  0.0496,  0.0983]],\n",
      "\n",
      "         [[-0.0510, -0.0080, -0.0531],\n",
      "          [-0.0037, -0.0185,  0.0228],\n",
      "          [-0.0315, -0.0577, -0.0110]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0092,  0.0722, -0.0626],\n",
      "          [-0.0966, -0.0792, -0.0792],\n",
      "          [ 0.0473,  0.0067, -0.0360]],\n",
      "\n",
      "         [[ 0.0284,  0.0335, -0.0110],\n",
      "          [ 0.0565,  0.0058,  0.0058],\n",
      "          [ 0.0308, -0.0808, -0.0733]],\n",
      "\n",
      "         [[ 0.0229,  0.1065,  0.0267],\n",
      "          [ 0.0297, -0.0568, -0.0339],\n",
      "          [ 0.0170,  0.0966, -0.0515]]],\n",
      "\n",
      "\n",
      "        [[[-0.0462,  0.0257, -0.0561],\n",
      "          [-0.0321,  0.0118, -0.0534],\n",
      "          [-0.0230, -0.0497, -0.0243]],\n",
      "\n",
      "         [[ 0.0549, -0.0648,  0.0614],\n",
      "          [ 0.0455, -0.0096,  0.0488],\n",
      "          [-0.0687, -0.0308, -0.0048]],\n",
      "\n",
      "         [[ 0.0642, -0.0840, -0.0862],\n",
      "          [-0.0421, -0.0662,  0.0574],\n",
      "          [ 0.0706,  0.0181,  0.0177]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0342,  0.0197,  0.0581],\n",
      "          [-0.0760, -0.0061,  0.0198],\n",
      "          [-0.0430, -0.0025, -0.0101]],\n",
      "\n",
      "         [[-0.0513, -0.0530, -0.0289],\n",
      "          [-0.0761, -0.0499, -0.0469],\n",
      "          [-0.0587, -0.0333, -0.0241]],\n",
      "\n",
      "         [[-0.0599, -0.0813,  0.0221],\n",
      "          [ 0.0594, -0.0012, -0.0873],\n",
      "          [ 0.0298,  0.0356,  0.0688]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.0649,  0.0596, -0.0307],\n",
      "          [-0.0250, -0.0242,  0.0869],\n",
      "          [-0.0404, -0.0196, -0.0973]],\n",
      "\n",
      "         [[ 0.0454,  0.0670,  0.0396],\n",
      "          [-0.0558, -0.0509,  0.0607],\n",
      "          [-0.0647,  0.0211,  0.0416]],\n",
      "\n",
      "         [[ 0.0661, -0.0106, -0.0252],\n",
      "          [-0.0826, -0.0491,  0.0238],\n",
      "          [ 0.0139,  0.0014, -0.0584]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0145, -0.0247, -0.0254],\n",
      "          [ 0.0798,  0.0501, -0.0004],\n",
      "          [ 0.0093,  0.0184,  0.0363]],\n",
      "\n",
      "         [[ 0.0307,  0.0675, -0.0461],\n",
      "          [ 0.0689, -0.0259, -0.0551],\n",
      "          [-0.0577,  0.0014, -0.0325]],\n",
      "\n",
      "         [[ 0.0008,  0.0223,  0.0128],\n",
      "          [ 0.0741, -0.0280,  0.0373],\n",
      "          [-0.0073, -0.0375, -0.1155]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0165, -0.0525, -0.0560],\n",
      "          [ 0.0659, -0.0489,  0.0675],\n",
      "          [-0.0316,  0.0240,  0.0653]],\n",
      "\n",
      "         [[-0.0329,  0.0065, -0.0119],\n",
      "          [-0.0456, -0.0477, -0.0551],\n",
      "          [-0.0946,  0.0215,  0.0523]],\n",
      "\n",
      "         [[-0.0813, -0.0173, -0.0157],\n",
      "          [-0.0066, -0.0564,  0.0540],\n",
      "          [ 0.0229, -0.0389, -0.0777]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0791, -0.0098, -0.0687],\n",
      "          [ 0.0007,  0.0466, -0.0359],\n",
      "          [ 0.0688, -0.0917, -0.0254]],\n",
      "\n",
      "         [[-0.0573, -0.0685, -0.0877],\n",
      "          [-0.0184, -0.0510,  0.0742],\n",
      "          [-0.0726,  0.0313, -0.0060]],\n",
      "\n",
      "         [[ 0.0682, -0.0709, -0.0741],\n",
      "          [-0.0462, -0.0229, -0.0051],\n",
      "          [ 0.0716, -0.0833, -0.0728]]],\n",
      "\n",
      "\n",
      "        [[[-0.0849, -0.0507, -0.0373],\n",
      "          [-0.0524, -0.0967,  0.0448],\n",
      "          [ 0.0883,  0.0419, -0.0928]],\n",
      "\n",
      "         [[-0.0582, -0.0405,  0.0609],\n",
      "          [ 0.0014, -0.0392,  0.0011],\n",
      "          [-0.0811, -0.0583, -0.0415]],\n",
      "\n",
      "         [[ 0.0134, -0.0609, -0.0015],\n",
      "          [-0.0711, -0.0222, -0.0522],\n",
      "          [ 0.0633,  0.0623,  0.0726]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0103,  0.0227, -0.0562],\n",
      "          [-0.0441, -0.0243, -0.0393],\n",
      "          [-0.0669,  0.0651,  0.0526]],\n",
      "\n",
      "         [[ 0.0807,  0.0407, -0.0465],\n",
      "          [ 0.0270, -0.0099, -0.0761],\n",
      "          [ 0.0241, -0.0857, -0.0057]],\n",
      "\n",
      "         [[-0.0830,  0.0324, -0.0664],\n",
      "          [ 0.0327, -0.0921, -0.0306],\n",
      "          [ 0.1073,  0.0530, -0.0170]]]], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-0.0558,  0.1132, -0.0797,  0.0487, -0.0099,  0.0647, -0.0175,  0.0496,\n",
      "         0.0539,  0.0837, -0.0045, -0.0303, -0.0479,  0.0045, -0.0329, -0.0516,\n",
      "         0.0618, -0.0205, -0.0387, -0.0098, -0.0451,  0.0764,  0.0531, -0.0606,\n",
      "        -0.0560, -0.0368, -0.0573, -0.0458,  0.0375, -0.0381,  0.0309,  0.0174],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([[[[ 0.0668]],\n",
      "\n",
      "         [[-0.1618]],\n",
      "\n",
      "         [[-0.0269]],\n",
      "\n",
      "         [[-0.1707]],\n",
      "\n",
      "         [[ 0.1305]],\n",
      "\n",
      "         [[-0.1290]],\n",
      "\n",
      "         [[-0.0938]],\n",
      "\n",
      "         [[-0.0003]],\n",
      "\n",
      "         [[ 0.1087]],\n",
      "\n",
      "         [[ 0.1059]],\n",
      "\n",
      "         [[ 0.0782]],\n",
      "\n",
      "         [[ 0.0036]],\n",
      "\n",
      "         [[-0.0945]],\n",
      "\n",
      "         [[-0.1634]],\n",
      "\n",
      "         [[ 0.1336]],\n",
      "\n",
      "         [[ 0.0386]],\n",
      "\n",
      "         [[ 0.1367]],\n",
      "\n",
      "         [[-0.1168]],\n",
      "\n",
      "         [[-0.0144]],\n",
      "\n",
      "         [[ 0.1508]],\n",
      "\n",
      "         [[-0.1150]],\n",
      "\n",
      "         [[ 0.1769]],\n",
      "\n",
      "         [[ 0.0310]],\n",
      "\n",
      "         [[-0.0804]],\n",
      "\n",
      "         [[-0.1223]],\n",
      "\n",
      "         [[-0.0867]],\n",
      "\n",
      "         [[ 0.0893]],\n",
      "\n",
      "         [[-0.1460]],\n",
      "\n",
      "         [[ 0.0327]],\n",
      "\n",
      "         [[ 0.0761]],\n",
      "\n",
      "         [[ 0.0899]],\n",
      "\n",
      "         [[-0.1093]]],\n",
      "\n",
      "\n",
      "        [[[-0.0272]],\n",
      "\n",
      "         [[ 0.1860]],\n",
      "\n",
      "         [[-0.0929]],\n",
      "\n",
      "         [[ 0.0101]],\n",
      "\n",
      "         [[-0.0539]],\n",
      "\n",
      "         [[-0.0119]],\n",
      "\n",
      "         [[-0.1592]],\n",
      "\n",
      "         [[-0.1399]],\n",
      "\n",
      "         [[-0.1582]],\n",
      "\n",
      "         [[ 0.1351]],\n",
      "\n",
      "         [[-0.1028]],\n",
      "\n",
      "         [[-0.1024]],\n",
      "\n",
      "         [[-0.0513]],\n",
      "\n",
      "         [[ 0.0972]],\n",
      "\n",
      "         [[ 0.1096]],\n",
      "\n",
      "         [[ 0.0043]],\n",
      "\n",
      "         [[ 0.0096]],\n",
      "\n",
      "         [[ 0.1648]],\n",
      "\n",
      "         [[-0.0467]],\n",
      "\n",
      "         [[ 0.0082]],\n",
      "\n",
      "         [[-0.1138]],\n",
      "\n",
      "         [[ 0.1628]],\n",
      "\n",
      "         [[-0.0436]],\n",
      "\n",
      "         [[ 0.0578]],\n",
      "\n",
      "         [[-0.1567]],\n",
      "\n",
      "         [[-0.0251]],\n",
      "\n",
      "         [[ 0.0794]],\n",
      "\n",
      "         [[ 0.0604]],\n",
      "\n",
      "         [[-0.1324]],\n",
      "\n",
      "         [[ 0.1644]],\n",
      "\n",
      "         [[-0.1254]],\n",
      "\n",
      "         [[ 0.2065]]],\n",
      "\n",
      "\n",
      "        [[[-0.0257]],\n",
      "\n",
      "         [[ 0.0066]],\n",
      "\n",
      "         [[ 0.1689]],\n",
      "\n",
      "         [[-0.1602]],\n",
      "\n",
      "         [[ 0.1516]],\n",
      "\n",
      "         [[-0.1624]],\n",
      "\n",
      "         [[ 0.1539]],\n",
      "\n",
      "         [[-0.0604]],\n",
      "\n",
      "         [[-0.1732]],\n",
      "\n",
      "         [[-0.0020]],\n",
      "\n",
      "         [[ 0.0891]],\n",
      "\n",
      "         [[ 0.0687]],\n",
      "\n",
      "         [[ 0.0859]],\n",
      "\n",
      "         [[ 0.0270]],\n",
      "\n",
      "         [[-0.0026]],\n",
      "\n",
      "         [[-0.0572]],\n",
      "\n",
      "         [[ 0.0082]],\n",
      "\n",
      "         [[-0.0844]],\n",
      "\n",
      "         [[-0.0513]],\n",
      "\n",
      "         [[ 0.0172]],\n",
      "\n",
      "         [[-0.1703]],\n",
      "\n",
      "         [[-0.0788]],\n",
      "\n",
      "         [[ 0.0751]],\n",
      "\n",
      "         [[ 0.1222]],\n",
      "\n",
      "         [[ 0.0457]],\n",
      "\n",
      "         [[ 0.0230]],\n",
      "\n",
      "         [[ 0.0270]],\n",
      "\n",
      "         [[-0.0083]],\n",
      "\n",
      "         [[ 0.1052]],\n",
      "\n",
      "         [[-0.0680]],\n",
      "\n",
      "         [[-0.0569]],\n",
      "\n",
      "         [[ 0.1086]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0234]],\n",
      "\n",
      "         [[-0.0989]],\n",
      "\n",
      "         [[ 0.0424]],\n",
      "\n",
      "         [[-0.1202]],\n",
      "\n",
      "         [[ 0.1182]],\n",
      "\n",
      "         [[ 0.1668]],\n",
      "\n",
      "         [[-0.1556]],\n",
      "\n",
      "         [[-0.0740]],\n",
      "\n",
      "         [[ 0.0058]],\n",
      "\n",
      "         [[ 0.0304]],\n",
      "\n",
      "         [[-0.1343]],\n",
      "\n",
      "         [[-0.1613]],\n",
      "\n",
      "         [[-0.0897]],\n",
      "\n",
      "         [[ 0.0755]],\n",
      "\n",
      "         [[-0.1222]],\n",
      "\n",
      "         [[ 0.1683]],\n",
      "\n",
      "         [[-0.1451]],\n",
      "\n",
      "         [[-0.1181]],\n",
      "\n",
      "         [[ 0.0015]],\n",
      "\n",
      "         [[-0.0323]],\n",
      "\n",
      "         [[ 0.0303]],\n",
      "\n",
      "         [[-0.1755]],\n",
      "\n",
      "         [[ 0.0792]],\n",
      "\n",
      "         [[ 0.0385]],\n",
      "\n",
      "         [[-0.0315]],\n",
      "\n",
      "         [[-0.1662]],\n",
      "\n",
      "         [[-0.1341]],\n",
      "\n",
      "         [[ 0.0011]],\n",
      "\n",
      "         [[ 0.0833]],\n",
      "\n",
      "         [[ 0.0331]],\n",
      "\n",
      "         [[ 0.0481]],\n",
      "\n",
      "         [[-0.0344]]],\n",
      "\n",
      "\n",
      "        [[[-0.1126]],\n",
      "\n",
      "         [[-0.0975]],\n",
      "\n",
      "         [[-0.0399]],\n",
      "\n",
      "         [[ 0.0922]],\n",
      "\n",
      "         [[ 0.1598]],\n",
      "\n",
      "         [[ 0.1539]],\n",
      "\n",
      "         [[ 0.1132]],\n",
      "\n",
      "         [[-0.0476]],\n",
      "\n",
      "         [[-0.1578]],\n",
      "\n",
      "         [[-0.0820]],\n",
      "\n",
      "         [[-0.1437]],\n",
      "\n",
      "         [[ 0.0705]],\n",
      "\n",
      "         [[-0.1388]],\n",
      "\n",
      "         [[ 0.1351]],\n",
      "\n",
      "         [[ 0.1540]],\n",
      "\n",
      "         [[-0.0769]],\n",
      "\n",
      "         [[ 0.0824]],\n",
      "\n",
      "         [[-0.1167]],\n",
      "\n",
      "         [[-0.1156]],\n",
      "\n",
      "         [[-0.1444]],\n",
      "\n",
      "         [[ 0.1332]],\n",
      "\n",
      "         [[-0.0056]],\n",
      "\n",
      "         [[-0.0630]],\n",
      "\n",
      "         [[ 0.1387]],\n",
      "\n",
      "         [[-0.0351]],\n",
      "\n",
      "         [[-0.0122]],\n",
      "\n",
      "         [[-0.1301]],\n",
      "\n",
      "         [[-0.0682]],\n",
      "\n",
      "         [[ 0.1772]],\n",
      "\n",
      "         [[-0.0661]],\n",
      "\n",
      "         [[ 0.0112]],\n",
      "\n",
      "         [[ 0.0816]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1023]],\n",
      "\n",
      "         [[-0.1112]],\n",
      "\n",
      "         [[-0.1122]],\n",
      "\n",
      "         [[-0.1506]],\n",
      "\n",
      "         [[ 0.0704]],\n",
      "\n",
      "         [[ 0.1247]],\n",
      "\n",
      "         [[-0.1471]],\n",
      "\n",
      "         [[-0.0378]],\n",
      "\n",
      "         [[-0.1648]],\n",
      "\n",
      "         [[-0.0916]],\n",
      "\n",
      "         [[ 0.1200]],\n",
      "\n",
      "         [[ 0.1294]],\n",
      "\n",
      "         [[-0.0239]],\n",
      "\n",
      "         [[-0.0978]],\n",
      "\n",
      "         [[ 0.0999]],\n",
      "\n",
      "         [[-0.0666]],\n",
      "\n",
      "         [[ 0.0230]],\n",
      "\n",
      "         [[ 0.0269]],\n",
      "\n",
      "         [[-0.0224]],\n",
      "\n",
      "         [[-0.0390]],\n",
      "\n",
      "         [[-0.0867]],\n",
      "\n",
      "         [[ 0.0514]],\n",
      "\n",
      "         [[-0.0154]],\n",
      "\n",
      "         [[ 0.0980]],\n",
      "\n",
      "         [[-0.1577]],\n",
      "\n",
      "         [[ 0.0624]],\n",
      "\n",
      "         [[-0.0277]],\n",
      "\n",
      "         [[ 0.1310]],\n",
      "\n",
      "         [[ 0.1229]],\n",
      "\n",
      "         [[ 0.1626]],\n",
      "\n",
      "         [[ 0.0029]],\n",
      "\n",
      "         [[-0.0514]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1727]],\n",
      "\n",
      "         [[ 0.1474]],\n",
      "\n",
      "         [[ 0.0578]],\n",
      "\n",
      "         [[ 0.1835]],\n",
      "\n",
      "         [[-0.0131]],\n",
      "\n",
      "         [[-0.1143]],\n",
      "\n",
      "         [[-0.1032]],\n",
      "\n",
      "         [[ 0.1663]],\n",
      "\n",
      "         [[ 0.1470]],\n",
      "\n",
      "         [[-0.0943]],\n",
      "\n",
      "         [[ 0.0861]],\n",
      "\n",
      "         [[-0.0278]],\n",
      "\n",
      "         [[-0.0733]],\n",
      "\n",
      "         [[-0.0485]],\n",
      "\n",
      "         [[ 0.1510]],\n",
      "\n",
      "         [[-0.1096]],\n",
      "\n",
      "         [[-0.1154]],\n",
      "\n",
      "         [[ 0.1691]],\n",
      "\n",
      "         [[ 0.1218]],\n",
      "\n",
      "         [[-0.0577]],\n",
      "\n",
      "         [[-0.1462]],\n",
      "\n",
      "         [[ 0.0537]],\n",
      "\n",
      "         [[-0.0240]],\n",
      "\n",
      "         [[-0.0478]],\n",
      "\n",
      "         [[ 0.0642]],\n",
      "\n",
      "         [[ 0.1028]],\n",
      "\n",
      "         [[-0.0356]],\n",
      "\n",
      "         [[-0.1043]],\n",
      "\n",
      "         [[-0.0177]],\n",
      "\n",
      "         [[ 0.1175]],\n",
      "\n",
      "         [[-0.0620]],\n",
      "\n",
      "         [[-0.1646]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0176]],\n",
      "\n",
      "         [[-0.0392]],\n",
      "\n",
      "         [[-0.0921]],\n",
      "\n",
      "         [[-0.0201]],\n",
      "\n",
      "         [[-0.1425]],\n",
      "\n",
      "         [[-0.1440]],\n",
      "\n",
      "         [[ 0.0004]],\n",
      "\n",
      "         [[-0.0217]],\n",
      "\n",
      "         [[ 0.1298]],\n",
      "\n",
      "         [[-0.1101]],\n",
      "\n",
      "         [[ 0.1004]],\n",
      "\n",
      "         [[-0.0144]],\n",
      "\n",
      "         [[-0.0906]],\n",
      "\n",
      "         [[ 0.1736]],\n",
      "\n",
      "         [[-0.0642]],\n",
      "\n",
      "         [[ 0.0470]],\n",
      "\n",
      "         [[ 0.0332]],\n",
      "\n",
      "         [[ 0.0458]],\n",
      "\n",
      "         [[-0.1113]],\n",
      "\n",
      "         [[ 0.0908]],\n",
      "\n",
      "         [[-0.0080]],\n",
      "\n",
      "         [[ 0.0074]],\n",
      "\n",
      "         [[ 0.1756]],\n",
      "\n",
      "         [[-0.1273]],\n",
      "\n",
      "         [[-0.1301]],\n",
      "\n",
      "         [[ 0.0497]],\n",
      "\n",
      "         [[ 0.0329]],\n",
      "\n",
      "         [[-0.0555]],\n",
      "\n",
      "         [[-0.1492]],\n",
      "\n",
      "         [[ 0.0043]],\n",
      "\n",
      "         [[-0.0506]],\n",
      "\n",
      "         [[ 0.0189]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0734]],\n",
      "\n",
      "         [[ 0.0608]],\n",
      "\n",
      "         [[ 0.1580]],\n",
      "\n",
      "         [[ 0.1564]],\n",
      "\n",
      "         [[-0.1774]],\n",
      "\n",
      "         [[-0.0086]],\n",
      "\n",
      "         [[ 0.0166]],\n",
      "\n",
      "         [[-0.0714]],\n",
      "\n",
      "         [[-0.0995]],\n",
      "\n",
      "         [[-0.0654]],\n",
      "\n",
      "         [[ 0.0276]],\n",
      "\n",
      "         [[-0.0446]],\n",
      "\n",
      "         [[ 0.0913]],\n",
      "\n",
      "         [[ 0.0819]],\n",
      "\n",
      "         [[ 0.0697]],\n",
      "\n",
      "         [[ 0.0228]],\n",
      "\n",
      "         [[ 0.0244]],\n",
      "\n",
      "         [[ 0.1275]],\n",
      "\n",
      "         [[ 0.1621]],\n",
      "\n",
      "         [[-0.1339]],\n",
      "\n",
      "         [[-0.1010]],\n",
      "\n",
      "         [[ 0.0126]],\n",
      "\n",
      "         [[-0.1898]],\n",
      "\n",
      "         [[-0.0942]],\n",
      "\n",
      "         [[ 0.1544]],\n",
      "\n",
      "         [[-0.0609]],\n",
      "\n",
      "         [[ 0.0918]],\n",
      "\n",
      "         [[-0.1437]],\n",
      "\n",
      "         [[ 0.1556]],\n",
      "\n",
      "         [[ 0.1108]],\n",
      "\n",
      "         [[ 0.1102]],\n",
      "\n",
      "         [[ 0.1075]]],\n",
      "\n",
      "\n",
      "        [[[-0.0675]],\n",
      "\n",
      "         [[-0.0100]],\n",
      "\n",
      "         [[-0.1172]],\n",
      "\n",
      "         [[ 0.1039]],\n",
      "\n",
      "         [[-0.0532]],\n",
      "\n",
      "         [[-0.0559]],\n",
      "\n",
      "         [[-0.0343]],\n",
      "\n",
      "         [[-0.0302]],\n",
      "\n",
      "         [[ 0.0752]],\n",
      "\n",
      "         [[ 0.1085]],\n",
      "\n",
      "         [[-0.0692]],\n",
      "\n",
      "         [[-0.1469]],\n",
      "\n",
      "         [[-0.0919]],\n",
      "\n",
      "         [[-0.0465]],\n",
      "\n",
      "         [[ 0.1649]],\n",
      "\n",
      "         [[ 0.0431]],\n",
      "\n",
      "         [[-0.0418]],\n",
      "\n",
      "         [[ 0.0927]],\n",
      "\n",
      "         [[-0.0834]],\n",
      "\n",
      "         [[-0.1572]],\n",
      "\n",
      "         [[ 0.0270]],\n",
      "\n",
      "         [[-0.0471]],\n",
      "\n",
      "         [[ 0.0179]],\n",
      "\n",
      "         [[ 0.1037]],\n",
      "\n",
      "         [[-0.0052]],\n",
      "\n",
      "         [[ 0.1674]],\n",
      "\n",
      "         [[-0.0486]],\n",
      "\n",
      "         [[ 0.1376]],\n",
      "\n",
      "         [[ 0.1287]],\n",
      "\n",
      "         [[ 0.1622]],\n",
      "\n",
      "         [[-0.1795]],\n",
      "\n",
      "         [[-0.0495]]],\n",
      "\n",
      "\n",
      "        [[[-0.1054]],\n",
      "\n",
      "         [[-0.1775]],\n",
      "\n",
      "         [[-0.0479]],\n",
      "\n",
      "         [[ 0.1671]],\n",
      "\n",
      "         [[ 0.1616]],\n",
      "\n",
      "         [[ 0.0632]],\n",
      "\n",
      "         [[ 0.1267]],\n",
      "\n",
      "         [[-0.1359]],\n",
      "\n",
      "         [[ 0.1523]],\n",
      "\n",
      "         [[ 0.0794]],\n",
      "\n",
      "         [[ 0.0199]],\n",
      "\n",
      "         [[-0.0621]],\n",
      "\n",
      "         [[ 0.1367]],\n",
      "\n",
      "         [[-0.0623]],\n",
      "\n",
      "         [[-0.1453]],\n",
      "\n",
      "         [[-0.0163]],\n",
      "\n",
      "         [[-0.0471]],\n",
      "\n",
      "         [[ 0.1383]],\n",
      "\n",
      "         [[-0.0611]],\n",
      "\n",
      "         [[-0.1753]],\n",
      "\n",
      "         [[-0.1757]],\n",
      "\n",
      "         [[-0.1646]],\n",
      "\n",
      "         [[-0.1484]],\n",
      "\n",
      "         [[-0.0300]],\n",
      "\n",
      "         [[-0.0612]],\n",
      "\n",
      "         [[-0.0552]],\n",
      "\n",
      "         [[ 0.1147]],\n",
      "\n",
      "         [[ 0.1187]],\n",
      "\n",
      "         [[ 0.0734]],\n",
      "\n",
      "         [[-0.1696]],\n",
      "\n",
      "         [[ 0.1539]],\n",
      "\n",
      "         [[-0.0104]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1234]],\n",
      "\n",
      "         [[-0.0694]],\n",
      "\n",
      "         [[-0.0347]],\n",
      "\n",
      "         [[-0.1718]],\n",
      "\n",
      "         [[-0.0602]],\n",
      "\n",
      "         [[ 0.1306]],\n",
      "\n",
      "         [[ 0.0880]],\n",
      "\n",
      "         [[ 0.0987]],\n",
      "\n",
      "         [[ 0.1472]],\n",
      "\n",
      "         [[-0.1116]],\n",
      "\n",
      "         [[-0.0411]],\n",
      "\n",
      "         [[-0.0796]],\n",
      "\n",
      "         [[-0.0842]],\n",
      "\n",
      "         [[-0.0361]],\n",
      "\n",
      "         [[ 0.1440]],\n",
      "\n",
      "         [[-0.1660]],\n",
      "\n",
      "         [[-0.1449]],\n",
      "\n",
      "         [[ 0.0059]],\n",
      "\n",
      "         [[-0.1483]],\n",
      "\n",
      "         [[ 0.0857]],\n",
      "\n",
      "         [[-0.0709]],\n",
      "\n",
      "         [[ 0.0748]],\n",
      "\n",
      "         [[-0.0840]],\n",
      "\n",
      "         [[-0.0154]],\n",
      "\n",
      "         [[-0.0131]],\n",
      "\n",
      "         [[-0.0559]],\n",
      "\n",
      "         [[ 0.1111]],\n",
      "\n",
      "         [[-0.0871]],\n",
      "\n",
      "         [[-0.1314]],\n",
      "\n",
      "         [[ 0.1096]],\n",
      "\n",
      "         [[-0.1262]],\n",
      "\n",
      "         [[ 0.0204]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1075]],\n",
      "\n",
      "         [[ 0.1823]],\n",
      "\n",
      "         [[-0.0551]],\n",
      "\n",
      "         [[ 0.0769]],\n",
      "\n",
      "         [[ 0.1204]],\n",
      "\n",
      "         [[-0.1028]],\n",
      "\n",
      "         [[-0.1086]],\n",
      "\n",
      "         [[-0.0187]],\n",
      "\n",
      "         [[-0.1030]],\n",
      "\n",
      "         [[ 0.1912]],\n",
      "\n",
      "         [[ 0.1024]],\n",
      "\n",
      "         [[-0.1255]],\n",
      "\n",
      "         [[-0.0083]],\n",
      "\n",
      "         [[-0.0008]],\n",
      "\n",
      "         [[ 0.1762]],\n",
      "\n",
      "         [[-0.0613]],\n",
      "\n",
      "         [[-0.1490]],\n",
      "\n",
      "         [[-0.1258]],\n",
      "\n",
      "         [[ 0.0956]],\n",
      "\n",
      "         [[-0.0349]],\n",
      "\n",
      "         [[ 0.1618]],\n",
      "\n",
      "         [[-0.0594]],\n",
      "\n",
      "         [[-0.1272]],\n",
      "\n",
      "         [[ 0.0527]],\n",
      "\n",
      "         [[ 0.0771]],\n",
      "\n",
      "         [[ 0.0539]],\n",
      "\n",
      "         [[-0.0929]],\n",
      "\n",
      "         [[ 0.1060]],\n",
      "\n",
      "         [[-0.1219]],\n",
      "\n",
      "         [[ 0.0886]],\n",
      "\n",
      "         [[-0.1293]],\n",
      "\n",
      "         [[ 0.0112]]],\n",
      "\n",
      "\n",
      "        [[[-0.0701]],\n",
      "\n",
      "         [[ 0.1066]],\n",
      "\n",
      "         [[-0.0406]],\n",
      "\n",
      "         [[-0.1696]],\n",
      "\n",
      "         [[ 0.1741]],\n",
      "\n",
      "         [[-0.1192]],\n",
      "\n",
      "         [[-0.0761]],\n",
      "\n",
      "         [[ 0.1688]],\n",
      "\n",
      "         [[-0.1572]],\n",
      "\n",
      "         [[-0.0276]],\n",
      "\n",
      "         [[-0.1767]],\n",
      "\n",
      "         [[ 0.0313]],\n",
      "\n",
      "         [[ 0.0121]],\n",
      "\n",
      "         [[ 0.0459]],\n",
      "\n",
      "         [[ 0.1009]],\n",
      "\n",
      "         [[ 0.0359]],\n",
      "\n",
      "         [[-0.0743]],\n",
      "\n",
      "         [[-0.1116]],\n",
      "\n",
      "         [[-0.0938]],\n",
      "\n",
      "         [[ 0.0146]],\n",
      "\n",
      "         [[-0.1590]],\n",
      "\n",
      "         [[-0.0900]],\n",
      "\n",
      "         [[ 0.1658]],\n",
      "\n",
      "         [[ 0.0269]],\n",
      "\n",
      "         [[-0.0282]],\n",
      "\n",
      "         [[-0.0190]],\n",
      "\n",
      "         [[-0.0612]],\n",
      "\n",
      "         [[ 0.0819]],\n",
      "\n",
      "         [[ 0.1031]],\n",
      "\n",
      "         [[-0.1519]],\n",
      "\n",
      "         [[ 0.1195]],\n",
      "\n",
      "         [[ 0.0583]]],\n",
      "\n",
      "\n",
      "        [[[-0.1677]],\n",
      "\n",
      "         [[-0.1310]],\n",
      "\n",
      "         [[-0.0735]],\n",
      "\n",
      "         [[-0.0979]],\n",
      "\n",
      "         [[-0.0862]],\n",
      "\n",
      "         [[-0.1733]],\n",
      "\n",
      "         [[-0.1545]],\n",
      "\n",
      "         [[-0.0795]],\n",
      "\n",
      "         [[-0.1255]],\n",
      "\n",
      "         [[-0.0128]],\n",
      "\n",
      "         [[ 0.0631]],\n",
      "\n",
      "         [[-0.0993]],\n",
      "\n",
      "         [[-0.0242]],\n",
      "\n",
      "         [[-0.1802]],\n",
      "\n",
      "         [[-0.0162]],\n",
      "\n",
      "         [[-0.0830]],\n",
      "\n",
      "         [[-0.1317]],\n",
      "\n",
      "         [[ 0.0387]],\n",
      "\n",
      "         [[ 0.0280]],\n",
      "\n",
      "         [[-0.0613]],\n",
      "\n",
      "         [[-0.1068]],\n",
      "\n",
      "         [[ 0.1625]],\n",
      "\n",
      "         [[-0.0320]],\n",
      "\n",
      "         [[ 0.0193]],\n",
      "\n",
      "         [[ 0.1295]],\n",
      "\n",
      "         [[ 0.0091]],\n",
      "\n",
      "         [[-0.1068]],\n",
      "\n",
      "         [[-0.0321]],\n",
      "\n",
      "         [[ 0.0754]],\n",
      "\n",
      "         [[-0.0253]],\n",
      "\n",
      "         [[-0.1017]],\n",
      "\n",
      "         [[-0.0901]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1744]],\n",
      "\n",
      "         [[-0.0055]],\n",
      "\n",
      "         [[-0.0977]],\n",
      "\n",
      "         [[ 0.0802]],\n",
      "\n",
      "         [[ 0.0124]],\n",
      "\n",
      "         [[ 0.1046]],\n",
      "\n",
      "         [[-0.1629]],\n",
      "\n",
      "         [[-0.0836]],\n",
      "\n",
      "         [[-0.1031]],\n",
      "\n",
      "         [[-0.1401]],\n",
      "\n",
      "         [[-0.0095]],\n",
      "\n",
      "         [[-0.0283]],\n",
      "\n",
      "         [[-0.1281]],\n",
      "\n",
      "         [[-0.0170]],\n",
      "\n",
      "         [[ 0.1220]],\n",
      "\n",
      "         [[-0.0569]],\n",
      "\n",
      "         [[-0.0751]],\n",
      "\n",
      "         [[ 0.0290]],\n",
      "\n",
      "         [[-0.0850]],\n",
      "\n",
      "         [[-0.0019]],\n",
      "\n",
      "         [[-0.1607]],\n",
      "\n",
      "         [[-0.0691]],\n",
      "\n",
      "         [[-0.1314]],\n",
      "\n",
      "         [[-0.0504]],\n",
      "\n",
      "         [[-0.1667]],\n",
      "\n",
      "         [[ 0.0745]],\n",
      "\n",
      "         [[-0.0662]],\n",
      "\n",
      "         [[ 0.0165]],\n",
      "\n",
      "         [[ 0.0733]],\n",
      "\n",
      "         [[ 0.0528]],\n",
      "\n",
      "         [[ 0.0771]],\n",
      "\n",
      "         [[ 0.0025]]]], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.1652, -0.0760,  0.1282, -0.1767,  0.1264,  0.0443,  0.0043,  0.1172,\n",
      "         0.1652,  0.0160, -0.1655,  0.0760, -0.0648, -0.0449,  0.1401, -0.1725],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([[[[-0.1031]],\n",
      "\n",
      "         [[-0.1686]],\n",
      "\n",
      "         [[ 0.0886]],\n",
      "\n",
      "         [[-0.0751]],\n",
      "\n",
      "         [[ 0.1542]],\n",
      "\n",
      "         [[-0.0559]],\n",
      "\n",
      "         [[ 0.1581]],\n",
      "\n",
      "         [[-0.0233]],\n",
      "\n",
      "         [[-0.1396]],\n",
      "\n",
      "         [[-0.0540]],\n",
      "\n",
      "         [[-0.1067]],\n",
      "\n",
      "         [[-0.0387]],\n",
      "\n",
      "         [[-0.0045]],\n",
      "\n",
      "         [[ 0.0337]],\n",
      "\n",
      "         [[ 0.0846]],\n",
      "\n",
      "         [[-0.0261]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0970]],\n",
      "\n",
      "         [[ 0.0019]],\n",
      "\n",
      "         [[ 0.1515]],\n",
      "\n",
      "         [[-0.1891]],\n",
      "\n",
      "         [[-0.1284]],\n",
      "\n",
      "         [[ 0.1034]],\n",
      "\n",
      "         [[-0.1908]],\n",
      "\n",
      "         [[-0.1600]],\n",
      "\n",
      "         [[ 0.0282]],\n",
      "\n",
      "         [[-0.0741]],\n",
      "\n",
      "         [[-0.0934]],\n",
      "\n",
      "         [[ 0.1466]],\n",
      "\n",
      "         [[-0.0041]],\n",
      "\n",
      "         [[-0.0407]],\n",
      "\n",
      "         [[-0.1003]],\n",
      "\n",
      "         [[ 0.0098]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2461]],\n",
      "\n",
      "         [[ 0.2019]],\n",
      "\n",
      "         [[ 0.0547]],\n",
      "\n",
      "         [[ 0.2098]],\n",
      "\n",
      "         [[ 0.0733]],\n",
      "\n",
      "         [[ 0.1688]],\n",
      "\n",
      "         [[-0.1480]],\n",
      "\n",
      "         [[-0.0764]],\n",
      "\n",
      "         [[-0.0219]],\n",
      "\n",
      "         [[ 0.0729]],\n",
      "\n",
      "         [[-0.1004]],\n",
      "\n",
      "         [[-0.1437]],\n",
      "\n",
      "         [[-0.1499]],\n",
      "\n",
      "         [[-0.0382]],\n",
      "\n",
      "         [[-0.1755]],\n",
      "\n",
      "         [[-0.1677]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1959]],\n",
      "\n",
      "         [[-0.0937]],\n",
      "\n",
      "         [[ 0.0802]],\n",
      "\n",
      "         [[ 0.1241]],\n",
      "\n",
      "         [[-0.1078]],\n",
      "\n",
      "         [[ 0.0012]],\n",
      "\n",
      "         [[-0.1277]],\n",
      "\n",
      "         [[ 0.2067]],\n",
      "\n",
      "         [[-0.0203]],\n",
      "\n",
      "         [[ 0.1340]],\n",
      "\n",
      "         [[-0.0866]],\n",
      "\n",
      "         [[ 0.1429]],\n",
      "\n",
      "         [[-0.2716]],\n",
      "\n",
      "         [[ 0.1767]],\n",
      "\n",
      "         [[-0.1559]],\n",
      "\n",
      "         [[ 0.2470]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2249]],\n",
      "\n",
      "         [[ 0.1225]],\n",
      "\n",
      "         [[ 0.1002]],\n",
      "\n",
      "         [[ 0.2472]],\n",
      "\n",
      "         [[-0.1261]],\n",
      "\n",
      "         [[-0.0993]],\n",
      "\n",
      "         [[-0.2459]],\n",
      "\n",
      "         [[-0.1253]],\n",
      "\n",
      "         [[ 0.0049]],\n",
      "\n",
      "         [[ 0.2234]],\n",
      "\n",
      "         [[ 0.1065]],\n",
      "\n",
      "         [[ 0.0863]],\n",
      "\n",
      "         [[-0.2677]],\n",
      "\n",
      "         [[ 0.2013]],\n",
      "\n",
      "         [[ 0.0952]],\n",
      "\n",
      "         [[ 0.0358]]],\n",
      "\n",
      "\n",
      "        [[[-0.2366]],\n",
      "\n",
      "         [[ 0.1099]],\n",
      "\n",
      "         [[-0.0461]],\n",
      "\n",
      "         [[-0.1656]],\n",
      "\n",
      "         [[-0.0648]],\n",
      "\n",
      "         [[ 0.0608]],\n",
      "\n",
      "         [[-0.0229]],\n",
      "\n",
      "         [[ 0.0432]],\n",
      "\n",
      "         [[ 0.1221]],\n",
      "\n",
      "         [[ 0.0731]],\n",
      "\n",
      "         [[-0.1239]],\n",
      "\n",
      "         [[ 0.1367]],\n",
      "\n",
      "         [[ 0.0686]],\n",
      "\n",
      "         [[ 0.1225]],\n",
      "\n",
      "         [[-0.2206]],\n",
      "\n",
      "         [[ 0.1755]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2201]],\n",
      "\n",
      "         [[-0.1254]],\n",
      "\n",
      "         [[-0.1153]],\n",
      "\n",
      "         [[ 0.0269]],\n",
      "\n",
      "         [[-0.0907]],\n",
      "\n",
      "         [[-0.0854]],\n",
      "\n",
      "         [[-0.1664]],\n",
      "\n",
      "         [[ 0.1434]],\n",
      "\n",
      "         [[-0.1388]],\n",
      "\n",
      "         [[ 0.0866]],\n",
      "\n",
      "         [[ 0.1032]],\n",
      "\n",
      "         [[ 0.1068]],\n",
      "\n",
      "         [[-0.1422]],\n",
      "\n",
      "         [[ 0.2132]],\n",
      "\n",
      "         [[-0.1468]],\n",
      "\n",
      "         [[ 0.2309]]],\n",
      "\n",
      "\n",
      "        [[[-0.1646]],\n",
      "\n",
      "         [[-0.1303]],\n",
      "\n",
      "         [[ 0.0560]],\n",
      "\n",
      "         [[-0.1518]],\n",
      "\n",
      "         [[-0.2522]],\n",
      "\n",
      "         [[ 0.0828]],\n",
      "\n",
      "         [[ 0.1019]],\n",
      "\n",
      "         [[-0.1440]],\n",
      "\n",
      "         [[ 0.0682]],\n",
      "\n",
      "         [[ 0.0150]],\n",
      "\n",
      "         [[ 0.2072]],\n",
      "\n",
      "         [[ 0.0376]],\n",
      "\n",
      "         [[-0.1011]],\n",
      "\n",
      "         [[-0.1250]],\n",
      "\n",
      "         [[ 0.0026]],\n",
      "\n",
      "         [[ 0.2165]]]], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-0.1691,  0.2022, -0.1719,  0.0923, -0.0016,  0.0533, -0.1513,  0.1082],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([[ 0.0199, -0.0079,  0.0170,  ..., -0.0043, -0.0079,  0.0072],\n",
      "        [ 0.0035, -0.0233, -0.0190,  ...,  0.0071, -0.0262,  0.0201],\n",
      "        [ 0.0168, -0.0027, -0.0017,  ...,  0.0117,  0.0254,  0.0336],\n",
      "        ...,\n",
      "        [-0.0305, -0.0213,  0.0214,  ..., -0.0229,  0.0163,  0.0274],\n",
      "        [-0.0187,  0.0027,  0.0001,  ..., -0.0209, -0.0218, -0.0264],\n",
      "        [ 0.0213, -0.0026, -0.0014,  ..., -0.0265, -0.0049, -0.0326]],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 2.7618e-02, -1.3550e-02, -4.4264e-03,  1.7035e-03,  1.6542e-02,\n",
      "        -5.5791e-03,  1.3712e-02, -1.4812e-02, -7.5759e-03,  3.6052e-03,\n",
      "         1.3603e-02, -2.9619e-03, -2.3982e-02,  6.7960e-03, -6.5590e-03,\n",
      "        -8.6824e-05,  1.0659e-03,  1.4288e-02,  7.7225e-03,  2.0126e-02,\n",
      "         1.7561e-02, -3.2127e-03, -4.9173e-03, -2.8745e-02,  1.7093e-02,\n",
      "        -1.8950e-02,  1.2101e-02,  1.2933e-02,  3.8407e-02,  8.3277e-03,\n",
      "         2.8116e-02,  2.3184e-02,  2.5276e-02,  8.4819e-03, -2.3746e-02,\n",
      "        -2.0828e-02, -2.9783e-02, -3.5491e-02,  2.4247e-02,  1.2092e-02,\n",
      "        -2.2917e-02, -2.1084e-02,  2.7647e-02,  5.7059e-03,  1.7070e-02,\n",
      "         2.8596e-03,  2.6647e-02, -1.7942e-02, -6.2783e-03,  1.1561e-02,\n",
      "        -1.5368e-02,  2.5780e-02,  3.9945e-02,  9.3004e-03,  8.4060e-03,\n",
      "        -2.9004e-02, -2.7008e-02,  2.0502e-02,  7.2049e-03,  2.1858e-03,\n",
      "         7.9749e-03, -1.6575e-02,  1.0303e-02,  2.2736e-02, -9.8939e-03,\n",
      "         3.5601e-03, -1.0204e-02, -1.2778e-02, -2.5584e-02,  2.3112e-02,\n",
      "        -3.1620e-02, -7.1694e-03,  2.9499e-02,  3.3804e-02,  3.2941e-02,\n",
      "        -2.5243e-02,  3.2010e-02,  1.8111e-02,  2.3385e-03, -1.6057e-02,\n",
      "         3.8038e-02,  1.7716e-02,  8.4822e-03, -3.4487e-02,  1.6831e-05,\n",
      "         5.7010e-04,  1.2131e-02, -2.6988e-02,  1.9534e-02, -3.5468e-02,\n",
      "        -2.0719e-02,  2.4881e-02, -1.4180e-02,  9.1213e-03, -5.7477e-03,\n",
      "        -2.0679e-02, -8.1591e-03, -2.8887e-02, -1.4311e-03,  3.5931e-02,\n",
      "         3.2702e-02, -2.5139e-02,  3.7813e-02,  3.0594e-02, -2.2393e-02,\n",
      "         3.7484e-03, -2.5740e-02,  7.9404e-04, -1.1516e-02, -3.6856e-02,\n",
      "         2.0582e-02,  2.4123e-02,  3.4481e-02,  1.2407e-02,  2.4624e-02,\n",
      "        -9.2581e-03,  2.4462e-02,  1.7981e-02,  3.1664e-02,  1.2320e-02,\n",
      "         1.9216e-02,  9.4940e-03,  2.6393e-02,  1.5102e-02, -2.8619e-02,\n",
      "        -1.3619e-02, -2.0418e-02,  2.8235e-03,  3.3444e-03,  2.9253e-02,\n",
      "        -1.9801e-02, -2.3947e-02,  3.3422e-02, -3.3648e-02,  1.5076e-02,\n",
      "         1.8905e-02,  3.5344e-02,  1.5328e-02, -2.2210e-02, -6.3395e-03,\n",
      "        -1.2634e-02, -2.3355e-02,  3.6109e-02, -2.8151e-02,  1.7394e-02,\n",
      "        -2.6234e-02,  1.1547e-02,  5.3690e-03,  8.3449e-05, -2.0228e-02,\n",
      "         7.3286e-03, -1.2757e-02, -2.8058e-02, -7.4180e-03,  5.1924e-03,\n",
      "        -1.9107e-02,  2.3266e-03, -9.2949e-03, -5.8625e-04,  6.8340e-03,\n",
      "        -6.4840e-03,  2.3930e-02, -2.3643e-02, -1.8786e-02, -2.4666e-02,\n",
      "        -8.7709e-03,  9.0794e-03,  3.6141e-02, -7.7293e-03,  2.5568e-02,\n",
      "         3.4157e-02,  2.8946e-03,  3.3420e-02, -2.3470e-02, -3.9408e-02,\n",
      "        -4.8795e-03, -8.7520e-03,  1.8492e-02, -6.1962e-03, -1.7412e-02,\n",
      "         1.1430e-02, -3.6000e-02,  1.7131e-02, -2.6067e-03, -3.8889e-02,\n",
      "         8.3920e-03, -2.1536e-03,  2.4149e-03,  2.8571e-02,  1.4874e-02,\n",
      "         1.4581e-02,  1.4699e-02, -3.0495e-02,  2.1579e-02,  2.1630e-02,\n",
      "         3.0726e-03,  1.5337e-02,  2.5519e-02, -8.7525e-03,  2.5616e-02,\n",
      "        -1.0049e-03, -2.8449e-02,  2.4480e-02,  2.0199e-02, -1.8280e-02,\n",
      "         2.4511e-02, -2.2953e-02, -2.5518e-02, -2.6191e-02, -1.4388e-02,\n",
      "        -3.3313e-02,  2.1640e-02, -2.1211e-02, -2.4014e-02, -1.9856e-02,\n",
      "         2.0021e-02,  3.2656e-02, -1.9749e-02, -1.5897e-02, -1.6421e-02,\n",
      "         8.3427e-03,  1.9773e-02,  2.0186e-02, -3.3861e-02,  1.7949e-02,\n",
      "        -3.2873e-02,  1.5419e-02,  4.2805e-03,  1.7838e-02,  5.5537e-03,\n",
      "         1.4447e-02,  9.8342e-03,  7.5865e-03,  2.7034e-02, -3.0679e-02,\n",
      "         2.9628e-02,  2.9792e-02, -2.9130e-02,  8.1884e-03,  6.7475e-03,\n",
      "         1.1945e-02,  2.4962e-02,  3.2386e-02, -3.5352e-02, -2.1819e-02,\n",
      "        -2.0526e-02,  1.0228e-03,  1.2003e-02,  9.1444e-03, -3.5714e-02,\n",
      "         2.4162e-05,  3.6061e-02, -1.5973e-02, -1.3125e-02, -1.8837e-02,\n",
      "        -2.2406e-02,  1.4735e-02, -2.0434e-02,  2.6010e-02,  2.9419e-02,\n",
      "         2.8171e-02, -3.8025e-02, -2.2498e-02,  5.7121e-04,  1.6998e-02,\n",
      "         1.8313e-02,  1.1998e-02,  8.5537e-03,  3.4565e-02,  1.9409e-02,\n",
      "         6.0766e-03,  7.5624e-03,  2.4099e-03,  6.8709e-03,  2.7416e-02,\n",
      "        -3.1271e-02, -1.7159e-03, -9.9835e-03, -7.9881e-03, -4.1441e-03,\n",
      "         1.2425e-02,  3.0606e-02, -3.7949e-02,  1.6662e-02,  1.6653e-02,\n",
      "         2.5562e-02, -1.5426e-02, -1.5463e-02,  1.1302e-02,  4.6540e-03,\n",
      "        -8.1467e-04,  2.2483e-02, -1.0788e-02,  3.1839e-02, -1.5000e-02,\n",
      "         2.6756e-03, -7.2804e-03, -2.2482e-02,  4.4276e-03, -3.2152e-02,\n",
      "        -1.4726e-02, -2.6043e-02, -2.7119e-02,  3.2361e-02, -2.4376e-02,\n",
      "        -2.1384e-02, -2.5256e-02,  2.9167e-02, -3.5307e-03, -1.6356e-02,\n",
      "         8.4144e-03,  3.4392e-04,  1.6256e-02, -1.8078e-02, -3.4418e-02,\n",
      "         2.7248e-02, -7.1234e-04,  1.5767e-02,  2.3960e-02, -1.7708e-02,\n",
      "        -3.8041e-03,  1.6006e-02,  1.7478e-02, -2.3563e-02,  3.1156e-02,\n",
      "         8.4182e-04,  2.0513e-02, -2.1284e-02, -3.1292e-02, -1.5823e-02,\n",
      "         2.7854e-03, -2.3122e-02, -1.9699e-02, -3.2020e-02, -5.3718e-03,\n",
      "         1.3625e-02,  2.5675e-02,  1.9916e-02,  2.2211e-03,  2.4040e-02,\n",
      "         1.3156e-02,  3.9341e-02,  1.5620e-02, -3.2837e-02, -2.0695e-02,\n",
      "        -6.5770e-03, -1.2134e-02, -2.0248e-02,  1.3008e-02, -1.4828e-02,\n",
      "         1.6186e-02,  3.3084e-02,  1.3347e-02,  8.9927e-03, -1.6328e-02,\n",
      "         6.2063e-03, -1.0081e-02, -1.1191e-02,  2.3459e-02, -1.8804e-02,\n",
      "        -8.2886e-03,  2.6070e-02,  3.5899e-02, -1.5621e-02,  1.3937e-02,\n",
      "        -1.3618e-02,  1.6308e-02,  4.0179e-02,  5.2026e-04,  8.5887e-03,\n",
      "        -2.2687e-02, -1.2092e-03, -2.0157e-02,  4.9001e-04,  3.3599e-04,\n",
      "        -1.9470e-02,  2.5810e-02, -5.3058e-03,  1.2931e-02,  1.8009e-02,\n",
      "         3.2529e-03,  2.8574e-02, -1.1890e-02, -2.0965e-02,  1.3136e-02,\n",
      "         2.5185e-02, -1.1762e-03, -1.7106e-02,  3.2551e-02, -2.1560e-02,\n",
      "        -3.4744e-02,  3.1172e-02, -9.2678e-03, -1.8324e-02,  2.7331e-02,\n",
      "        -2.2924e-02,  2.9091e-03, -2.1378e-02, -1.2079e-02,  9.4811e-03,\n",
      "        -7.3861e-03, -3.5413e-02, -3.0813e-02,  1.1851e-02, -2.4825e-02,\n",
      "         1.6114e-02,  3.3347e-02, -5.3994e-03, -2.1492e-03,  2.6241e-02,\n",
      "         2.7159e-03,  1.9267e-02,  2.8637e-03,  1.8249e-02, -5.1083e-03,\n",
      "        -1.8553e-02, -1.0851e-02,  1.4260e-02, -2.0093e-02, -2.0764e-02,\n",
      "         6.4184e-03, -1.2224e-02,  8.6635e-03,  5.7838e-04,  3.1326e-02,\n",
      "         1.8675e-02,  3.2597e-02,  2.6270e-02, -1.9128e-02, -4.6416e-03,\n",
      "         7.6842e-03,  8.1295e-03,  1.5246e-02,  1.1321e-02,  2.3643e-02,\n",
      "         1.4010e-02, -3.2724e-02, -3.1493e-02, -1.0357e-02, -3.9013e-02,\n",
      "        -7.6774e-03,  9.6400e-04, -9.5647e-03, -1.1832e-02, -2.6124e-02,\n",
      "        -1.1769e-02,  3.1044e-02,  8.5745e-03, -3.6817e-03,  1.7302e-02,\n",
      "         4.9436e-03,  2.7296e-02,  1.3230e-02, -4.3547e-03,  3.1811e-02,\n",
      "         3.3734e-02,  3.5777e-02, -8.1771e-03,  1.7990e-02, -9.0106e-03,\n",
      "        -1.9530e-02,  6.6426e-03,  4.1909e-03, -1.3092e-02, -1.7031e-02,\n",
      "        -1.4903e-02,  2.9933e-03, -8.9286e-03,  3.3935e-03, -3.4324e-02,\n",
      "        -1.6011e-02, -2.8151e-02, -1.2890e-02, -2.5415e-02,  8.9155e-03,\n",
      "         8.9199e-03,  2.5043e-02,  6.9381e-03, -1.6444e-02, -2.2914e-03,\n",
      "         2.6014e-02,  2.5898e-02, -6.2855e-03,  2.2820e-02,  2.3491e-02,\n",
      "        -1.1331e-02,  4.1830e-03,  9.5742e-06, -2.0800e-02,  2.9384e-02,\n",
      "         3.6989e-02, -2.0725e-03,  6.9572e-03, -2.3101e-03, -2.3960e-02,\n",
      "        -1.5390e-02,  4.8672e-03,  1.2792e-02,  2.6532e-02, -1.9676e-02,\n",
      "        -5.5360e-03, -2.7553e-03,  1.1288e-02,  3.5885e-02, -2.2484e-02,\n",
      "        -8.9399e-04, -1.3033e-02,  1.6635e-02, -3.8314e-02, -4.5512e-03,\n",
      "        -2.1378e-02, -2.0027e-02], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([[-0.0365,  0.0275,  0.0094,  ..., -0.0074,  0.0152,  0.0384],\n",
      "        [ 0.0013, -0.0337,  0.0121,  ..., -0.0024, -0.0231,  0.0277]],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0129, -0.0255], device='cuda:0', requires_grad=True)]\n",
      "123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[[[-0.1283, -0.1933, -0.2767],\n",
      "          [ 0.3537,  0.0489,  0.2934],\n",
      "          [ 0.0899, -0.3423, -0.3517]],\n",
      "\n",
      "         [[-0.3329, -0.2808,  0.1242],\n",
      "          [-0.3232, -0.0019,  0.2211],\n",
      "          [-0.0702, -0.3318, -0.3270]],\n",
      "\n",
      "         [[ 0.1867, -0.2396, -0.1101],\n",
      "          [-0.2081,  0.2469,  0.1123],\n",
      "          [-0.2431,  0.2779, -0.3564]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0093, -0.1351, -0.2346],\n",
      "          [-0.2610,  0.1387, -0.2568],\n",
      "          [ 0.3335, -0.0204,  0.0514]],\n",
      "\n",
      "         [[-0.0443,  0.0894, -0.1484],\n",
      "          [-0.2531,  0.1929, -0.0247],\n",
      "          [-0.2901, -0.3322,  0.0639]],\n",
      "\n",
      "         [[-0.1936, -0.2634,  0.1761],\n",
      "          [ 0.2563,  0.1899, -0.2442],\n",
      "          [ 0.2932,  0.3348,  0.1239]]],\n",
      "\n",
      "\n",
      "        [[[-0.2586,  0.1480, -0.0441],\n",
      "          [ 0.2705, -0.0844, -0.0636],\n",
      "          [-0.0225,  0.3081, -0.3151]],\n",
      "\n",
      "         [[-0.2371,  0.0935, -0.3550],\n",
      "          [-0.2158,  0.0426,  0.0848],\n",
      "          [ 0.3793,  0.0617,  0.3796]],\n",
      "\n",
      "         [[ 0.0242,  0.3905,  0.2780],\n",
      "          [-0.1818,  0.0571, -0.3525],\n",
      "          [-0.3306, -0.2159,  0.0662]]],\n",
      "\n",
      "\n",
      "        [[[-0.1847,  0.3058, -0.1047],\n",
      "          [ 0.0299, -0.1730, -0.1102],\n",
      "          [ 0.3491, -0.2296, -0.3450]],\n",
      "\n",
      "         [[ 0.0810, -0.2781,  0.1349],\n",
      "          [-0.0440, -0.1995,  0.2618],\n",
      "          [ 0.1027,  0.3255,  0.1746]],\n",
      "\n",
      "         [[-0.3004,  0.2497,  0.1514],\n",
      "          [-0.0487,  0.3761,  0.3158],\n",
      "          [ 0.4032,  0.1540,  0.4098]]],\n",
      "\n",
      "\n",
      "        [[[-0.2463, -0.1769, -0.2070],\n",
      "          [ 0.2437,  0.0286,  0.0271],\n",
      "          [ 0.4129,  0.1313, -0.1918]],\n",
      "\n",
      "         [[-0.3264, -0.1192, -0.1134],\n",
      "          [ 0.4442,  0.3447, -0.0817],\n",
      "          [ 0.0760,  0.3256, -0.1617]],\n",
      "\n",
      "         [[ 0.3576,  0.4328,  0.1301],\n",
      "          [ 0.0276,  0.1553,  0.3276],\n",
      "          [ 0.1795, -0.0050, -0.3157]]],\n",
      "\n",
      "\n",
      "        [[[-0.0628, -0.0071,  0.3131],\n",
      "          [ 0.1564,  0.0870, -0.2335],\n",
      "          [-0.1221,  0.1647,  0.2635]],\n",
      "\n",
      "         [[ 0.3098, -0.0290, -0.2557],\n",
      "          [-0.0963,  0.3952,  0.3312],\n",
      "          [-0.0557, -0.2322, -0.1930]],\n",
      "\n",
      "         [[ 0.1796,  0.1569,  0.1187],\n",
      "          [ 0.2282,  0.2951,  0.1760],\n",
      "          [-0.0174,  0.0540,  0.1617]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0171, -0.0193,  0.0291],\n",
      "          [ 0.0479, -0.3056,  0.3388],\n",
      "          [-0.0467,  0.0341,  0.0192]],\n",
      "\n",
      "         [[ 0.0531,  0.1125, -0.1438],\n",
      "          [-0.2910,  0.2774,  0.0562],\n",
      "          [-0.2125,  0.0272, -0.2563]],\n",
      "\n",
      "         [[-0.1887,  0.3604,  0.4035],\n",
      "          [-0.2941, -0.3557, -0.1196],\n",
      "          [-0.1715, -0.3390, -0.0788]]],\n",
      "\n",
      "\n",
      "        [[[-0.2654,  0.1816,  0.0498],\n",
      "          [-0.2522, -0.1403,  0.1751],\n",
      "          [-0.2259, -0.2672, -0.3280]],\n",
      "\n",
      "         [[ 0.3186,  0.3344,  0.3636],\n",
      "          [-0.0281,  0.3080,  0.1312],\n",
      "          [-0.2777, -0.2657,  0.3583]],\n",
      "\n",
      "         [[ 0.1308, -0.3109,  0.0753],\n",
      "          [-0.1488,  0.1468,  0.2258],\n",
      "          [-0.1734, -0.0677, -0.1568]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2831,  0.3624,  0.2510],\n",
      "          [-0.2820,  0.0464, -0.1780],\n",
      "          [-0.2190,  0.0415,  0.2994]],\n",
      "\n",
      "         [[ 0.1398,  0.3597, -0.2457],\n",
      "          [-0.3278,  0.1374, -0.1713],\n",
      "          [ 0.0170,  0.0411,  0.0300]],\n",
      "\n",
      "         [[ 0.0429, -0.1994, -0.2041],\n",
      "          [ 0.3539, -0.1336, -0.3374],\n",
      "          [-0.2295, -0.3843,  0.3466]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0084, -0.1215, -0.1019],\n",
      "          [-0.2749,  0.1104, -0.2792],\n",
      "          [ 0.2057,  0.1770,  0.0038]],\n",
      "\n",
      "         [[ 0.2192,  0.0141, -0.2047],\n",
      "          [-0.2835, -0.2447,  0.0867],\n",
      "          [-0.0891,  0.0323, -0.0258]],\n",
      "\n",
      "         [[ 0.0019,  0.1438,  0.3442],\n",
      "          [ 0.1746, -0.1854, -0.3117],\n",
      "          [-0.1034,  0.2731,  0.1558]]],\n",
      "\n",
      "\n",
      "        [[[-0.2384,  0.2677,  0.1994],\n",
      "          [ 0.1307,  0.0292,  0.4168],\n",
      "          [ 0.2373,  0.4269,  0.3238]],\n",
      "\n",
      "         [[ 0.0353,  0.2229,  0.3098],\n",
      "          [ 0.4389, -0.1346, -0.1472],\n",
      "          [ 0.3102, -0.3148,  0.3536]],\n",
      "\n",
      "         [[ 0.1134, -0.0018,  0.0760],\n",
      "          [ 0.1214,  0.1619,  0.1002],\n",
      "          [ 0.2340,  0.2548, -0.1083]]],\n",
      "\n",
      "\n",
      "        [[[-0.1556,  0.1975, -0.3162],\n",
      "          [-0.2001,  0.1295,  0.2233],\n",
      "          [ 0.1253, -0.1234,  0.1122]],\n",
      "\n",
      "         [[ 0.2066,  0.1279,  0.3566],\n",
      "          [ 0.3234,  0.2040, -0.0406],\n",
      "          [-0.1812,  0.3879,  0.2419]],\n",
      "\n",
      "         [[-0.3420,  0.3380, -0.0178],\n",
      "          [-0.0042, -0.3657, -0.0405],\n",
      "          [ 0.2703,  0.3047,  0.1720]]],\n",
      "\n",
      "\n",
      "        [[[-0.3778,  0.2448, -0.0979],\n",
      "          [-0.2305, -0.0083,  0.3112],\n",
      "          [ 0.1625,  0.3223,  0.3063]],\n",
      "\n",
      "         [[ 0.3710, -0.0267, -0.0373],\n",
      "          [-0.1531,  0.0498, -0.1310],\n",
      "          [ 0.0080,  0.1797,  0.2213]],\n",
      "\n",
      "         [[ 0.1136,  0.3875, -0.0913],\n",
      "          [-0.3167,  0.1594, -0.0542],\n",
      "          [-0.0839,  0.2375,  0.1787]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0143, -0.1208,  0.1958],\n",
      "          [ 0.0042,  0.0029,  0.0669],\n",
      "          [-0.0868, -0.2263, -0.1939]],\n",
      "\n",
      "         [[ 0.1766,  0.1194, -0.1313],\n",
      "          [ 0.2500,  0.2717,  0.2803],\n",
      "          [-0.2178, -0.2570, -0.0131]],\n",
      "\n",
      "         [[-0.1660,  0.3816,  0.1853],\n",
      "          [-0.3724, -0.1559,  0.1850],\n",
      "          [ 0.1432, -0.3557,  0.0341]]],\n",
      "\n",
      "\n",
      "        [[[-0.1465, -0.0197, -0.3134],\n",
      "          [-0.0018,  0.1042, -0.0671],\n",
      "          [ 0.3265,  0.3036, -0.2617]],\n",
      "\n",
      "         [[-0.3038, -0.3256,  0.2398],\n",
      "          [ 0.0235,  0.3288,  0.0437],\n",
      "          [-0.2935, -0.0074, -0.2976]],\n",
      "\n",
      "         [[-0.1643, -0.3126,  0.2883],\n",
      "          [ 0.0282,  0.3894,  0.1609],\n",
      "          [ 0.3401,  0.2573, -0.1088]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1702,  0.2692, -0.1812],\n",
      "          [ 0.1535,  0.1933, -0.2716],\n",
      "          [-0.2756,  0.1613,  0.1831]],\n",
      "\n",
      "         [[ 0.2027, -0.4007, -0.2612],\n",
      "          [-0.0929,  0.0340,  0.0521],\n",
      "          [-0.0756, -0.1314, -0.1719]],\n",
      "\n",
      "         [[-0.3491,  0.0179,  0.2824],\n",
      "          [ 0.0920,  0.0277,  0.1130],\n",
      "          [ 0.0179,  0.2615,  0.0126]]]], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.2946,  0.3115, -0.2741, -0.2923,  0.2933,  0.2177, -0.3007,  0.0734,\n",
      "        -0.2746, -0.2246,  0.1468,  0.0376, -0.1474,  0.2176,  0.1076, -0.3329],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([[[[ 0.0826,  0.0765,  0.1453],\n",
      "          [ 0.0289,  0.0217,  0.0667],\n",
      "          [ 0.1046, -0.1432,  0.1365]],\n",
      "\n",
      "         [[ 0.0896, -0.1018,  0.1354],\n",
      "          [-0.0157, -0.0881,  0.0961],\n",
      "          [ 0.0189, -0.0297,  0.1309]],\n",
      "\n",
      "         [[ 0.1634,  0.1517, -0.0449],\n",
      "          [-0.0472, -0.0056,  0.0623],\n",
      "          [-0.0311,  0.1139,  0.0579]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.1371,  0.1402, -0.0333],\n",
      "          [ 0.1779, -0.0558, -0.0935],\n",
      "          [ 0.1819,  0.0591,  0.0282]],\n",
      "\n",
      "         [[ 0.0885, -0.0360, -0.0823],\n",
      "          [ 0.1098,  0.0904,  0.0206],\n",
      "          [ 0.0820, -0.0655,  0.0967]],\n",
      "\n",
      "         [[-0.0253,  0.0021,  0.0663],\n",
      "          [ 0.0690,  0.2039, -0.0274],\n",
      "          [-0.1030,  0.2669, -0.0650]]],\n",
      "\n",
      "\n",
      "        [[[-0.0918, -0.0842, -0.0939],\n",
      "          [ 0.1211,  0.0694,  0.1335],\n",
      "          [ 0.0299, -0.0514,  0.1442]],\n",
      "\n",
      "         [[ 0.1387,  0.0474,  0.1131],\n",
      "          [-0.1480, -0.1304, -0.1189],\n",
      "          [-0.0517, -0.1252, -0.0839]],\n",
      "\n",
      "         [[ 0.0812,  0.1028,  0.0015],\n",
      "          [ 0.0561,  0.1388,  0.1128],\n",
      "          [-0.1554, -0.1351,  0.1518]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0284,  0.0021, -0.0128],\n",
      "          [ 0.0121, -0.0316, -0.1164],\n",
      "          [ 0.0957, -0.0013,  0.0679]],\n",
      "\n",
      "         [[-0.0387,  0.1102,  0.0995],\n",
      "          [-0.1404,  0.0247,  0.0995],\n",
      "          [ 0.1227, -0.0632,  0.0201]],\n",
      "\n",
      "         [[-0.1224,  0.0871,  0.0702],\n",
      "          [ 0.0671, -0.0184,  0.1650],\n",
      "          [-0.0857, -0.1559,  0.1589]]],\n",
      "\n",
      "\n",
      "        [[[-0.0143, -0.1631,  0.0293],\n",
      "          [-0.1069,  0.0083,  0.1230],\n",
      "          [-0.1705,  0.0825, -0.1008]],\n",
      "\n",
      "         [[ 0.0050,  0.0834, -0.1536],\n",
      "          [-0.0214, -0.1386, -0.1288],\n",
      "          [-0.0617,  0.1324,  0.1275]],\n",
      "\n",
      "         [[-0.0731,  0.0665,  0.0087],\n",
      "          [ 0.1958,  0.0504, -0.0502],\n",
      "          [ 0.0820,  0.0397, -0.0116]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0410,  0.0191, -0.1632],\n",
      "          [-0.0244, -0.0682,  0.1499],\n",
      "          [ 0.0864, -0.0815, -0.0730]],\n",
      "\n",
      "         [[-0.1246, -0.0230,  0.0537],\n",
      "          [ 0.0191, -0.1299,  0.0432],\n",
      "          [ 0.0461,  0.0056, -0.0637]],\n",
      "\n",
      "         [[ 0.1159, -0.1693, -0.0173],\n",
      "          [ 0.0042,  0.0694,  0.0722],\n",
      "          [-0.0806,  0.0827,  0.0541]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.1342,  0.0319,  0.0255],\n",
      "          [-0.1677,  0.0713, -0.0758],\n",
      "          [ 0.1073, -0.1686,  0.1051]],\n",
      "\n",
      "         [[ 0.0897, -0.0726,  0.1378],\n",
      "          [-0.0105, -0.1064,  0.0946],\n",
      "          [ 0.0747,  0.1289, -0.1672]],\n",
      "\n",
      "         [[-0.0211,  0.0726, -0.0161],\n",
      "          [ 0.1394, -0.0681, -0.0066],\n",
      "          [-0.0639, -0.0684,  0.0752]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.1701,  0.0844,  0.1139],\n",
      "          [-0.1318, -0.0642,  0.0461],\n",
      "          [-0.1802,  0.1236, -0.1572]],\n",
      "\n",
      "         [[-0.0045, -0.0141, -0.1790],\n",
      "          [-0.1164,  0.0882, -0.1398],\n",
      "          [-0.0482,  0.0501,  0.0320]],\n",
      "\n",
      "         [[ 0.0047, -0.1690,  0.0847],\n",
      "          [-0.0125,  0.0170,  0.1115],\n",
      "          [-0.1233, -0.0157, -0.0103]]],\n",
      "\n",
      "\n",
      "        [[[-0.0179,  0.0950, -0.0089],\n",
      "          [-0.1175, -0.0629, -0.0667],\n",
      "          [ 0.0989,  0.1323, -0.1349]],\n",
      "\n",
      "         [[-0.0398, -0.1236,  0.1309],\n",
      "          [ 0.0385, -0.1026, -0.0580],\n",
      "          [ 0.0586,  0.0818,  0.0559]],\n",
      "\n",
      "         [[-0.0338, -0.0033, -0.1207],\n",
      "          [-0.1765, -0.1158,  0.0172],\n",
      "          [-0.0092, -0.1183, -0.1721]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0433, -0.0870, -0.1082],\n",
      "          [-0.1365,  0.0551, -0.1753],\n",
      "          [ 0.1034, -0.0667,  0.1129]],\n",
      "\n",
      "         [[-0.1245,  0.0158,  0.0887],\n",
      "          [-0.0661,  0.0582,  0.1343],\n",
      "          [ 0.1217, -0.0871, -0.0304]],\n",
      "\n",
      "         [[ 0.0195, -0.0916,  0.0841],\n",
      "          [ 0.0542,  0.0668, -0.1364],\n",
      "          [-0.1587,  0.0477,  0.0838]]],\n",
      "\n",
      "\n",
      "        [[[-0.0858,  0.0783, -0.1391],\n",
      "          [ 0.0222,  0.1527, -0.1060],\n",
      "          [-0.0581, -0.1443, -0.1089]],\n",
      "\n",
      "         [[ 0.0260, -0.0993,  0.0566],\n",
      "          [-0.0471, -0.0645,  0.1242],\n",
      "          [ 0.0469,  0.1168,  0.0084]],\n",
      "\n",
      "         [[-0.0695, -0.0447, -0.0304],\n",
      "          [ 0.1170,  0.0616,  0.0987],\n",
      "          [-0.1677,  0.1043, -0.0911]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0194,  0.0846,  0.0729],\n",
      "          [-0.1159,  0.1154,  0.0659],\n",
      "          [-0.1438, -0.0429,  0.0708]],\n",
      "\n",
      "         [[-0.0630, -0.1341,  0.0369],\n",
      "          [ 0.0968, -0.1473, -0.1083],\n",
      "          [-0.0743, -0.0895,  0.1350]],\n",
      "\n",
      "         [[ 0.0691, -0.0530,  0.1145],\n",
      "          [-0.0194,  0.0938, -0.1738],\n",
      "          [-0.1371,  0.1185, -0.0157]]]], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.1611, -0.0661, -0.1616,  0.0222, -0.0040, -0.0515, -0.0010,  0.1065,\n",
      "        -0.1739,  0.1551, -0.0792,  0.0520,  0.0526,  0.1734,  0.1318, -0.0670,\n",
      "        -0.0174,  0.0366,  0.0986,  0.0688,  0.0886,  0.1063,  0.1659, -0.1423,\n",
      "        -0.0097, -0.1443,  0.0199, -0.0712, -0.0252, -0.1554,  0.0440, -0.1268],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([[[[ 0.2887]],\n",
      "\n",
      "         [[ 0.1175]],\n",
      "\n",
      "         [[-0.3065]],\n",
      "\n",
      "         [[ 0.2656]],\n",
      "\n",
      "         [[-0.0594]],\n",
      "\n",
      "         [[ 0.2238]],\n",
      "\n",
      "         [[-0.0896]],\n",
      "\n",
      "         [[-0.1473]],\n",
      "\n",
      "         [[ 0.1655]],\n",
      "\n",
      "         [[ 0.2503]],\n",
      "\n",
      "         [[ 0.2494]],\n",
      "\n",
      "         [[-0.0291]],\n",
      "\n",
      "         [[ 0.0167]],\n",
      "\n",
      "         [[-0.3202]],\n",
      "\n",
      "         [[-0.1910]],\n",
      "\n",
      "         [[-0.0663]],\n",
      "\n",
      "         [[ 0.2439]],\n",
      "\n",
      "         [[ 0.0568]],\n",
      "\n",
      "         [[ 0.1814]],\n",
      "\n",
      "         [[ 0.3241]],\n",
      "\n",
      "         [[ 0.0117]],\n",
      "\n",
      "         [[-0.3685]],\n",
      "\n",
      "         [[-0.2287]],\n",
      "\n",
      "         [[ 0.3120]],\n",
      "\n",
      "         [[ 0.1383]],\n",
      "\n",
      "         [[ 0.1314]],\n",
      "\n",
      "         [[-0.2030]],\n",
      "\n",
      "         [[ 0.2780]],\n",
      "\n",
      "         [[ 0.1084]],\n",
      "\n",
      "         [[ 0.1417]],\n",
      "\n",
      "         [[ 0.0312]],\n",
      "\n",
      "         [[-0.2635]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0126]],\n",
      "\n",
      "         [[ 0.0254]],\n",
      "\n",
      "         [[-0.2325]],\n",
      "\n",
      "         [[-0.1982]],\n",
      "\n",
      "         [[ 0.1098]],\n",
      "\n",
      "         [[-0.0143]],\n",
      "\n",
      "         [[ 0.1066]],\n",
      "\n",
      "         [[ 0.1011]],\n",
      "\n",
      "         [[ 0.0516]],\n",
      "\n",
      "         [[-0.1778]],\n",
      "\n",
      "         [[ 0.0596]],\n",
      "\n",
      "         [[ 0.2291]],\n",
      "\n",
      "         [[-0.2801]],\n",
      "\n",
      "         [[ 0.2969]],\n",
      "\n",
      "         [[ 0.1738]],\n",
      "\n",
      "         [[ 0.1793]],\n",
      "\n",
      "         [[ 0.2327]],\n",
      "\n",
      "         [[ 0.1534]],\n",
      "\n",
      "         [[ 0.3215]],\n",
      "\n",
      "         [[ 0.1882]],\n",
      "\n",
      "         [[-0.0218]],\n",
      "\n",
      "         [[-0.0650]],\n",
      "\n",
      "         [[ 0.2490]],\n",
      "\n",
      "         [[ 0.0444]],\n",
      "\n",
      "         [[-0.1082]],\n",
      "\n",
      "         [[ 0.1074]],\n",
      "\n",
      "         [[-0.2886]],\n",
      "\n",
      "         [[ 0.0720]],\n",
      "\n",
      "         [[-0.1225]],\n",
      "\n",
      "         [[ 0.0189]],\n",
      "\n",
      "         [[ 0.0351]],\n",
      "\n",
      "         [[ 0.1117]]],\n",
      "\n",
      "\n",
      "        [[[-0.1619]],\n",
      "\n",
      "         [[-0.1204]],\n",
      "\n",
      "         [[-0.2790]],\n",
      "\n",
      "         [[ 0.2114]],\n",
      "\n",
      "         [[ 0.3084]],\n",
      "\n",
      "         [[ 0.3012]],\n",
      "\n",
      "         [[-0.1389]],\n",
      "\n",
      "         [[ 0.2791]],\n",
      "\n",
      "         [[ 0.0932]],\n",
      "\n",
      "         [[-0.1114]],\n",
      "\n",
      "         [[ 0.0443]],\n",
      "\n",
      "         [[ 0.1195]],\n",
      "\n",
      "         [[ 0.2824]],\n",
      "\n",
      "         [[-0.0414]],\n",
      "\n",
      "         [[-0.3068]],\n",
      "\n",
      "         [[-0.1923]],\n",
      "\n",
      "         [[-0.2455]],\n",
      "\n",
      "         [[ 0.2382]],\n",
      "\n",
      "         [[-0.3628]],\n",
      "\n",
      "         [[ 0.0541]],\n",
      "\n",
      "         [[ 0.1960]],\n",
      "\n",
      "         [[-0.2940]],\n",
      "\n",
      "         [[ 0.3035]],\n",
      "\n",
      "         [[ 0.2126]],\n",
      "\n",
      "         [[ 0.0802]],\n",
      "\n",
      "         [[ 0.0435]],\n",
      "\n",
      "         [[ 0.1683]],\n",
      "\n",
      "         [[ 0.1572]],\n",
      "\n",
      "         [[-0.1963]],\n",
      "\n",
      "         [[-0.1203]],\n",
      "\n",
      "         [[ 0.1068]],\n",
      "\n",
      "         [[ 0.3225]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1071]],\n",
      "\n",
      "         [[ 0.1463]],\n",
      "\n",
      "         [[ 0.1091]],\n",
      "\n",
      "         [[ 0.2940]],\n",
      "\n",
      "         [[ 0.0384]],\n",
      "\n",
      "         [[ 0.0746]],\n",
      "\n",
      "         [[ 0.0350]],\n",
      "\n",
      "         [[ 0.2979]],\n",
      "\n",
      "         [[ 0.3298]],\n",
      "\n",
      "         [[-0.0566]],\n",
      "\n",
      "         [[ 0.2128]],\n",
      "\n",
      "         [[-0.2700]],\n",
      "\n",
      "         [[-0.0885]],\n",
      "\n",
      "         [[ 0.0184]],\n",
      "\n",
      "         [[ 0.3333]],\n",
      "\n",
      "         [[ 0.3204]],\n",
      "\n",
      "         [[ 0.3562]],\n",
      "\n",
      "         [[ 0.3462]],\n",
      "\n",
      "         [[ 0.2858]],\n",
      "\n",
      "         [[ 0.0571]],\n",
      "\n",
      "         [[-0.3186]],\n",
      "\n",
      "         [[ 0.0067]],\n",
      "\n",
      "         [[ 0.3374]],\n",
      "\n",
      "         [[ 0.3390]],\n",
      "\n",
      "         [[-0.1865]],\n",
      "\n",
      "         [[-0.1119]],\n",
      "\n",
      "         [[-0.2442]],\n",
      "\n",
      "         [[ 0.1687]],\n",
      "\n",
      "         [[-0.2089]],\n",
      "\n",
      "         [[ 0.3166]],\n",
      "\n",
      "         [[ 0.2072]],\n",
      "\n",
      "         [[-0.3227]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0865]],\n",
      "\n",
      "         [[-0.2672]],\n",
      "\n",
      "         [[ 0.0595]],\n",
      "\n",
      "         [[-0.2016]],\n",
      "\n",
      "         [[ 0.0429]],\n",
      "\n",
      "         [[ 0.1733]],\n",
      "\n",
      "         [[-0.2165]],\n",
      "\n",
      "         [[-0.2523]],\n",
      "\n",
      "         [[-0.3091]],\n",
      "\n",
      "         [[-0.3925]],\n",
      "\n",
      "         [[-0.1708]],\n",
      "\n",
      "         [[-0.1668]],\n",
      "\n",
      "         [[ 0.1786]],\n",
      "\n",
      "         [[ 0.1467]],\n",
      "\n",
      "         [[ 0.0108]],\n",
      "\n",
      "         [[ 0.1154]],\n",
      "\n",
      "         [[-0.1568]],\n",
      "\n",
      "         [[-0.1129]],\n",
      "\n",
      "         [[-0.2304]],\n",
      "\n",
      "         [[-0.1032]],\n",
      "\n",
      "         [[-0.0075]],\n",
      "\n",
      "         [[-0.1721]],\n",
      "\n",
      "         [[ 0.0328]],\n",
      "\n",
      "         [[ 0.0289]],\n",
      "\n",
      "         [[-0.2531]],\n",
      "\n",
      "         [[ 0.3220]],\n",
      "\n",
      "         [[ 0.2726]],\n",
      "\n",
      "         [[ 0.1727]],\n",
      "\n",
      "         [[ 0.2768]],\n",
      "\n",
      "         [[ 0.2723]],\n",
      "\n",
      "         [[-0.3195]],\n",
      "\n",
      "         [[ 0.2760]]],\n",
      "\n",
      "\n",
      "        [[[-0.1327]],\n",
      "\n",
      "         [[-0.2036]],\n",
      "\n",
      "         [[ 0.1727]],\n",
      "\n",
      "         [[ 0.2822]],\n",
      "\n",
      "         [[ 0.0776]],\n",
      "\n",
      "         [[ 0.1792]],\n",
      "\n",
      "         [[-0.1489]],\n",
      "\n",
      "         [[-0.1193]],\n",
      "\n",
      "         [[ 0.1878]],\n",
      "\n",
      "         [[-0.1076]],\n",
      "\n",
      "         [[ 0.2766]],\n",
      "\n",
      "         [[ 0.2895]],\n",
      "\n",
      "         [[-0.2613]],\n",
      "\n",
      "         [[ 0.1275]],\n",
      "\n",
      "         [[ 0.0553]],\n",
      "\n",
      "         [[-0.1905]],\n",
      "\n",
      "         [[-0.2417]],\n",
      "\n",
      "         [[-0.3233]],\n",
      "\n",
      "         [[ 0.3331]],\n",
      "\n",
      "         [[ 0.3140]],\n",
      "\n",
      "         [[ 0.2480]],\n",
      "\n",
      "         [[-0.0921]],\n",
      "\n",
      "         [[-0.2246]],\n",
      "\n",
      "         [[-0.0245]],\n",
      "\n",
      "         [[ 0.2020]],\n",
      "\n",
      "         [[-0.3389]],\n",
      "\n",
      "         [[-0.1206]],\n",
      "\n",
      "         [[ 0.1070]],\n",
      "\n",
      "         [[ 0.2878]],\n",
      "\n",
      "         [[ 0.1696]],\n",
      "\n",
      "         [[ 0.1155]],\n",
      "\n",
      "         [[-0.0774]]],\n",
      "\n",
      "\n",
      "        [[[-0.0818]],\n",
      "\n",
      "         [[ 0.1695]],\n",
      "\n",
      "         [[-0.0470]],\n",
      "\n",
      "         [[ 0.2462]],\n",
      "\n",
      "         [[ 0.3288]],\n",
      "\n",
      "         [[ 0.1716]],\n",
      "\n",
      "         [[ 0.1628]],\n",
      "\n",
      "         [[ 0.1283]],\n",
      "\n",
      "         [[-0.0562]],\n",
      "\n",
      "         [[-0.2337]],\n",
      "\n",
      "         [[-0.1255]],\n",
      "\n",
      "         [[-0.2667]],\n",
      "\n",
      "         [[-0.0932]],\n",
      "\n",
      "         [[ 0.1726]],\n",
      "\n",
      "         [[ 0.1503]],\n",
      "\n",
      "         [[-0.1086]],\n",
      "\n",
      "         [[ 0.0881]],\n",
      "\n",
      "         [[ 0.0581]],\n",
      "\n",
      "         [[ 0.1985]],\n",
      "\n",
      "         [[-0.3154]],\n",
      "\n",
      "         [[ 0.2767]],\n",
      "\n",
      "         [[ 0.3275]],\n",
      "\n",
      "         [[ 0.3639]],\n",
      "\n",
      "         [[ 0.3396]],\n",
      "\n",
      "         [[-0.2687]],\n",
      "\n",
      "         [[-0.1309]],\n",
      "\n",
      "         [[-0.0723]],\n",
      "\n",
      "         [[-0.1097]],\n",
      "\n",
      "         [[ 0.1348]],\n",
      "\n",
      "         [[ 0.2386]],\n",
      "\n",
      "         [[ 0.1846]],\n",
      "\n",
      "         [[-0.2042]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2319]],\n",
      "\n",
      "         [[-0.3158]],\n",
      "\n",
      "         [[-0.2047]],\n",
      "\n",
      "         [[-0.0974]],\n",
      "\n",
      "         [[-0.2497]],\n",
      "\n",
      "         [[ 0.3346]],\n",
      "\n",
      "         [[ 0.3606]],\n",
      "\n",
      "         [[-0.2734]],\n",
      "\n",
      "         [[-0.0941]],\n",
      "\n",
      "         [[ 0.3401]],\n",
      "\n",
      "         [[ 0.0624]],\n",
      "\n",
      "         [[-0.0713]],\n",
      "\n",
      "         [[ 0.3427]],\n",
      "\n",
      "         [[ 0.1500]],\n",
      "\n",
      "         [[-0.1670]],\n",
      "\n",
      "         [[-0.3420]],\n",
      "\n",
      "         [[ 0.3424]],\n",
      "\n",
      "         [[-0.0222]],\n",
      "\n",
      "         [[ 0.2550]],\n",
      "\n",
      "         [[-0.1185]],\n",
      "\n",
      "         [[ 0.1576]],\n",
      "\n",
      "         [[ 0.3059]],\n",
      "\n",
      "         [[-0.1247]],\n",
      "\n",
      "         [[ 0.2972]],\n",
      "\n",
      "         [[-0.0063]],\n",
      "\n",
      "         [[ 0.2091]],\n",
      "\n",
      "         [[-0.3279]],\n",
      "\n",
      "         [[-0.1177]],\n",
      "\n",
      "         [[-0.2837]],\n",
      "\n",
      "         [[ 0.0638]],\n",
      "\n",
      "         [[-0.0477]],\n",
      "\n",
      "         [[-0.0156]]],\n",
      "\n",
      "\n",
      "        [[[-0.0180]],\n",
      "\n",
      "         [[ 0.2345]],\n",
      "\n",
      "         [[ 0.1970]],\n",
      "\n",
      "         [[-0.2556]],\n",
      "\n",
      "         [[-0.2163]],\n",
      "\n",
      "         [[ 0.1703]],\n",
      "\n",
      "         [[-0.1644]],\n",
      "\n",
      "         [[ 0.1923]],\n",
      "\n",
      "         [[-0.0595]],\n",
      "\n",
      "         [[-0.3131]],\n",
      "\n",
      "         [[-0.0134]],\n",
      "\n",
      "         [[-0.3443]],\n",
      "\n",
      "         [[ 0.2863]],\n",
      "\n",
      "         [[-0.2060]],\n",
      "\n",
      "         [[ 0.3238]],\n",
      "\n",
      "         [[ 0.2948]],\n",
      "\n",
      "         [[ 0.0737]],\n",
      "\n",
      "         [[-0.2714]],\n",
      "\n",
      "         [[-0.1140]],\n",
      "\n",
      "         [[ 0.0273]],\n",
      "\n",
      "         [[ 0.1427]],\n",
      "\n",
      "         [[ 0.1840]],\n",
      "\n",
      "         [[-0.3101]],\n",
      "\n",
      "         [[ 0.1856]],\n",
      "\n",
      "         [[-0.3287]],\n",
      "\n",
      "         [[-0.1599]],\n",
      "\n",
      "         [[ 0.0603]],\n",
      "\n",
      "         [[ 0.1384]],\n",
      "\n",
      "         [[ 0.2811]],\n",
      "\n",
      "         [[ 0.0832]],\n",
      "\n",
      "         [[-0.0904]],\n",
      "\n",
      "         [[-0.3030]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1537]],\n",
      "\n",
      "         [[-0.3426]],\n",
      "\n",
      "         [[-0.0503]],\n",
      "\n",
      "         [[ 0.0968]],\n",
      "\n",
      "         [[-0.0221]],\n",
      "\n",
      "         [[-0.2118]],\n",
      "\n",
      "         [[ 0.1977]],\n",
      "\n",
      "         [[-0.3646]],\n",
      "\n",
      "         [[ 0.2345]],\n",
      "\n",
      "         [[-0.0845]],\n",
      "\n",
      "         [[ 0.3085]],\n",
      "\n",
      "         [[ 0.2257]],\n",
      "\n",
      "         [[-0.0874]],\n",
      "\n",
      "         [[-0.1044]],\n",
      "\n",
      "         [[-0.3332]],\n",
      "\n",
      "         [[ 0.1817]],\n",
      "\n",
      "         [[-0.1536]],\n",
      "\n",
      "         [[ 0.2654]],\n",
      "\n",
      "         [[ 0.3348]],\n",
      "\n",
      "         [[-0.3225]],\n",
      "\n",
      "         [[ 0.1812]],\n",
      "\n",
      "         [[-0.0566]],\n",
      "\n",
      "         [[-0.1146]],\n",
      "\n",
      "         [[-0.1289]],\n",
      "\n",
      "         [[ 0.0132]],\n",
      "\n",
      "         [[-0.2782]],\n",
      "\n",
      "         [[-0.0299]],\n",
      "\n",
      "         [[ 0.2684]],\n",
      "\n",
      "         [[-0.0355]],\n",
      "\n",
      "         [[ 0.1969]],\n",
      "\n",
      "         [[ 0.0915]],\n",
      "\n",
      "         [[ 0.2005]]],\n",
      "\n",
      "\n",
      "        [[[-0.1175]],\n",
      "\n",
      "         [[-0.2397]],\n",
      "\n",
      "         [[-0.1497]],\n",
      "\n",
      "         [[ 0.0379]],\n",
      "\n",
      "         [[ 0.0285]],\n",
      "\n",
      "         [[ 0.0230]],\n",
      "\n",
      "         [[-0.0724]],\n",
      "\n",
      "         [[-0.2938]],\n",
      "\n",
      "         [[ 0.2577]],\n",
      "\n",
      "         [[ 0.2060]],\n",
      "\n",
      "         [[ 0.0334]],\n",
      "\n",
      "         [[ 0.2653]],\n",
      "\n",
      "         [[ 0.0320]],\n",
      "\n",
      "         [[ 0.3222]],\n",
      "\n",
      "         [[ 0.0289]],\n",
      "\n",
      "         [[ 0.3646]],\n",
      "\n",
      "         [[ 0.0488]],\n",
      "\n",
      "         [[ 0.0969]],\n",
      "\n",
      "         [[ 0.1388]],\n",
      "\n",
      "         [[ 0.2611]],\n",
      "\n",
      "         [[-0.3441]],\n",
      "\n",
      "         [[ 0.2838]],\n",
      "\n",
      "         [[-0.1662]],\n",
      "\n",
      "         [[ 0.0698]],\n",
      "\n",
      "         [[ 0.1699]],\n",
      "\n",
      "         [[ 0.1334]],\n",
      "\n",
      "         [[-0.0335]],\n",
      "\n",
      "         [[-0.2283]],\n",
      "\n",
      "         [[ 0.0155]],\n",
      "\n",
      "         [[-0.0880]],\n",
      "\n",
      "         [[ 0.1246]],\n",
      "\n",
      "         [[-0.2659]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3935]],\n",
      "\n",
      "         [[-0.1436]],\n",
      "\n",
      "         [[ 0.0298]],\n",
      "\n",
      "         [[-0.1741]],\n",
      "\n",
      "         [[ 0.0575]],\n",
      "\n",
      "         [[-0.0485]],\n",
      "\n",
      "         [[ 0.1474]],\n",
      "\n",
      "         [[-0.2664]],\n",
      "\n",
      "         [[ 0.3413]],\n",
      "\n",
      "         [[-0.0842]],\n",
      "\n",
      "         [[-0.2633]],\n",
      "\n",
      "         [[-0.3528]],\n",
      "\n",
      "         [[ 0.1044]],\n",
      "\n",
      "         [[ 0.3306]],\n",
      "\n",
      "         [[ 0.1836]],\n",
      "\n",
      "         [[ 0.1689]],\n",
      "\n",
      "         [[-0.3562]],\n",
      "\n",
      "         [[-0.3847]],\n",
      "\n",
      "         [[-0.3417]],\n",
      "\n",
      "         [[ 0.3395]],\n",
      "\n",
      "         [[-0.0936]],\n",
      "\n",
      "         [[-0.1265]],\n",
      "\n",
      "         [[ 0.2669]],\n",
      "\n",
      "         [[-0.0703]],\n",
      "\n",
      "         [[-0.2068]],\n",
      "\n",
      "         [[ 0.4008]],\n",
      "\n",
      "         [[ 0.1808]],\n",
      "\n",
      "         [[-0.2636]],\n",
      "\n",
      "         [[ 0.0137]],\n",
      "\n",
      "         [[ 0.1826]],\n",
      "\n",
      "         [[-0.1979]],\n",
      "\n",
      "         [[-0.0141]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0204]],\n",
      "\n",
      "         [[ 0.1995]],\n",
      "\n",
      "         [[-0.0540]],\n",
      "\n",
      "         [[-0.0324]],\n",
      "\n",
      "         [[ 0.0688]],\n",
      "\n",
      "         [[-0.2950]],\n",
      "\n",
      "         [[-0.3282]],\n",
      "\n",
      "         [[ 0.0679]],\n",
      "\n",
      "         [[ 0.1966]],\n",
      "\n",
      "         [[-0.3122]],\n",
      "\n",
      "         [[ 0.2473]],\n",
      "\n",
      "         [[-0.2299]],\n",
      "\n",
      "         [[-0.3854]],\n",
      "\n",
      "         [[ 0.2744]],\n",
      "\n",
      "         [[-0.3410]],\n",
      "\n",
      "         [[ 0.2521]],\n",
      "\n",
      "         [[ 0.2243]],\n",
      "\n",
      "         [[-0.0576]],\n",
      "\n",
      "         [[-0.1830]],\n",
      "\n",
      "         [[ 0.2862]],\n",
      "\n",
      "         [[-0.1874]],\n",
      "\n",
      "         [[ 0.1324]],\n",
      "\n",
      "         [[-0.1442]],\n",
      "\n",
      "         [[-0.3133]],\n",
      "\n",
      "         [[-0.2225]],\n",
      "\n",
      "         [[ 0.0134]],\n",
      "\n",
      "         [[-0.1287]],\n",
      "\n",
      "         [[ 0.0170]],\n",
      "\n",
      "         [[-0.1171]],\n",
      "\n",
      "         [[ 0.0576]],\n",
      "\n",
      "         [[-0.2898]],\n",
      "\n",
      "         [[-0.1968]]],\n",
      "\n",
      "\n",
      "        [[[-0.3010]],\n",
      "\n",
      "         [[ 0.1038]],\n",
      "\n",
      "         [[-0.2910]],\n",
      "\n",
      "         [[ 0.0414]],\n",
      "\n",
      "         [[-0.1281]],\n",
      "\n",
      "         [[-0.1371]],\n",
      "\n",
      "         [[ 0.1263]],\n",
      "\n",
      "         [[ 0.1121]],\n",
      "\n",
      "         [[-0.0918]],\n",
      "\n",
      "         [[ 0.1536]],\n",
      "\n",
      "         [[-0.2603]],\n",
      "\n",
      "         [[ 0.2088]],\n",
      "\n",
      "         [[-0.2823]],\n",
      "\n",
      "         [[-0.2141]],\n",
      "\n",
      "         [[-0.2887]],\n",
      "\n",
      "         [[ 0.0224]],\n",
      "\n",
      "         [[ 0.1569]],\n",
      "\n",
      "         [[ 0.2071]],\n",
      "\n",
      "         [[ 0.1202]],\n",
      "\n",
      "         [[ 0.0202]],\n",
      "\n",
      "         [[ 0.2655]],\n",
      "\n",
      "         [[-0.2746]],\n",
      "\n",
      "         [[-0.0182]],\n",
      "\n",
      "         [[ 0.2976]],\n",
      "\n",
      "         [[-0.1983]],\n",
      "\n",
      "         [[ 0.3229]],\n",
      "\n",
      "         [[-0.1148]],\n",
      "\n",
      "         [[ 0.0687]],\n",
      "\n",
      "         [[-0.2720]],\n",
      "\n",
      "         [[-0.1326]],\n",
      "\n",
      "         [[ 0.2681]],\n",
      "\n",
      "         [[-0.2404]]],\n",
      "\n",
      "\n",
      "        [[[-0.1423]],\n",
      "\n",
      "         [[ 0.1709]],\n",
      "\n",
      "         [[-0.0061]],\n",
      "\n",
      "         [[-0.0897]],\n",
      "\n",
      "         [[ 0.3582]],\n",
      "\n",
      "         [[ 0.1788]],\n",
      "\n",
      "         [[-0.1132]],\n",
      "\n",
      "         [[-0.2200]],\n",
      "\n",
      "         [[-0.1196]],\n",
      "\n",
      "         [[-0.2185]],\n",
      "\n",
      "         [[-0.0138]],\n",
      "\n",
      "         [[ 0.0547]],\n",
      "\n",
      "         [[-0.3348]],\n",
      "\n",
      "         [[ 0.3346]],\n",
      "\n",
      "         [[ 0.1581]],\n",
      "\n",
      "         [[ 0.2027]],\n",
      "\n",
      "         [[ 0.3312]],\n",
      "\n",
      "         [[-0.2790]],\n",
      "\n",
      "         [[ 0.1932]],\n",
      "\n",
      "         [[ 0.1644]],\n",
      "\n",
      "         [[-0.1037]],\n",
      "\n",
      "         [[-0.0822]],\n",
      "\n",
      "         [[ 0.2937]],\n",
      "\n",
      "         [[ 0.2270]],\n",
      "\n",
      "         [[ 0.3412]],\n",
      "\n",
      "         [[-0.2337]],\n",
      "\n",
      "         [[-0.2770]],\n",
      "\n",
      "         [[ 0.2492]],\n",
      "\n",
      "         [[ 0.2813]],\n",
      "\n",
      "         [[-0.2336]],\n",
      "\n",
      "         [[ 0.0015]],\n",
      "\n",
      "         [[-0.1412]]],\n",
      "\n",
      "\n",
      "        [[[-0.3352]],\n",
      "\n",
      "         [[ 0.0408]],\n",
      "\n",
      "         [[-0.0346]],\n",
      "\n",
      "         [[ 0.2406]],\n",
      "\n",
      "         [[-0.0987]],\n",
      "\n",
      "         [[ 0.3712]],\n",
      "\n",
      "         [[-0.1060]],\n",
      "\n",
      "         [[-0.2334]],\n",
      "\n",
      "         [[ 0.0016]],\n",
      "\n",
      "         [[-0.2397]],\n",
      "\n",
      "         [[-0.1750]],\n",
      "\n",
      "         [[ 0.3280]],\n",
      "\n",
      "         [[-0.0422]],\n",
      "\n",
      "         [[-0.0910]],\n",
      "\n",
      "         [[ 0.2123]],\n",
      "\n",
      "         [[ 0.1840]],\n",
      "\n",
      "         [[-0.0700]],\n",
      "\n",
      "         [[ 0.1710]],\n",
      "\n",
      "         [[ 0.1259]],\n",
      "\n",
      "         [[-0.0247]],\n",
      "\n",
      "         [[-0.3608]],\n",
      "\n",
      "         [[ 0.1363]],\n",
      "\n",
      "         [[ 0.3471]],\n",
      "\n",
      "         [[ 0.2839]],\n",
      "\n",
      "         [[-0.2091]],\n",
      "\n",
      "         [[-0.1940]],\n",
      "\n",
      "         [[ 0.1811]],\n",
      "\n",
      "         [[-0.1156]],\n",
      "\n",
      "         [[-0.1333]],\n",
      "\n",
      "         [[ 0.0139]],\n",
      "\n",
      "         [[ 0.0307]],\n",
      "\n",
      "         [[ 0.0649]]]], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.2985,  0.2526, -0.1643, -0.1762, -0.0355, -0.2344, -0.2237,  0.1817,\n",
      "        -0.2324,  0.0213, -0.0803, -0.0904, -0.2155, -0.0932,  0.2424,  0.2170],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([[[[ 0.1482]],\n",
      "\n",
      "         [[-0.1973]],\n",
      "\n",
      "         [[-0.1341]],\n",
      "\n",
      "         [[ 0.0799]],\n",
      "\n",
      "         [[-0.1810]],\n",
      "\n",
      "         [[-0.4464]],\n",
      "\n",
      "         [[ 0.4649]],\n",
      "\n",
      "         [[ 0.4848]],\n",
      "\n",
      "         [[-0.1261]],\n",
      "\n",
      "         [[-0.1578]],\n",
      "\n",
      "         [[-0.3992]],\n",
      "\n",
      "         [[ 0.0636]],\n",
      "\n",
      "         [[ 0.3746]],\n",
      "\n",
      "         [[ 0.0334]],\n",
      "\n",
      "         [[ 0.1209]],\n",
      "\n",
      "         [[ 0.0364]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1702]],\n",
      "\n",
      "         [[-0.2761]],\n",
      "\n",
      "         [[-0.2284]],\n",
      "\n",
      "         [[-0.3869]],\n",
      "\n",
      "         [[-0.2494]],\n",
      "\n",
      "         [[ 0.3847]],\n",
      "\n",
      "         [[ 0.1178]],\n",
      "\n",
      "         [[ 0.2564]],\n",
      "\n",
      "         [[-0.0109]],\n",
      "\n",
      "         [[-0.1362]],\n",
      "\n",
      "         [[-0.1191]],\n",
      "\n",
      "         [[ 0.3906]],\n",
      "\n",
      "         [[ 0.0130]],\n",
      "\n",
      "         [[-0.0669]],\n",
      "\n",
      "         [[-0.0700]],\n",
      "\n",
      "         [[-0.4489]]],\n",
      "\n",
      "\n",
      "        [[[-0.0013]],\n",
      "\n",
      "         [[-0.0098]],\n",
      "\n",
      "         [[-0.0089]],\n",
      "\n",
      "         [[-0.2217]],\n",
      "\n",
      "         [[ 0.3451]],\n",
      "\n",
      "         [[-0.0284]],\n",
      "\n",
      "         [[ 0.3864]],\n",
      "\n",
      "         [[-0.0225]],\n",
      "\n",
      "         [[-0.0783]],\n",
      "\n",
      "         [[-0.1959]],\n",
      "\n",
      "         [[-0.3003]],\n",
      "\n",
      "         [[-0.2158]],\n",
      "\n",
      "         [[-0.2477]],\n",
      "\n",
      "         [[-0.4668]],\n",
      "\n",
      "         [[ 0.4509]],\n",
      "\n",
      "         [[ 0.3866]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3848]],\n",
      "\n",
      "         [[-0.3879]],\n",
      "\n",
      "         [[-0.2388]],\n",
      "\n",
      "         [[-0.2154]],\n",
      "\n",
      "         [[-0.4217]],\n",
      "\n",
      "         [[-0.2276]],\n",
      "\n",
      "         [[ 0.2572]],\n",
      "\n",
      "         [[ 0.0035]],\n",
      "\n",
      "         [[-0.1975]],\n",
      "\n",
      "         [[ 0.2740]],\n",
      "\n",
      "         [[ 0.0153]],\n",
      "\n",
      "         [[ 0.0067]],\n",
      "\n",
      "         [[ 0.3725]],\n",
      "\n",
      "         [[ 0.0938]],\n",
      "\n",
      "         [[ 0.0742]],\n",
      "\n",
      "         [[ 0.1494]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1932]],\n",
      "\n",
      "         [[-0.4545]],\n",
      "\n",
      "         [[ 0.2099]],\n",
      "\n",
      "         [[-0.2475]],\n",
      "\n",
      "         [[-0.4086]],\n",
      "\n",
      "         [[-0.1284]],\n",
      "\n",
      "         [[ 0.4655]],\n",
      "\n",
      "         [[ 0.0823]],\n",
      "\n",
      "         [[-0.3957]],\n",
      "\n",
      "         [[ 0.2034]],\n",
      "\n",
      "         [[-0.3808]],\n",
      "\n",
      "         [[ 0.4848]],\n",
      "\n",
      "         [[ 0.0847]],\n",
      "\n",
      "         [[ 0.1329]],\n",
      "\n",
      "         [[ 0.3316]],\n",
      "\n",
      "         [[-0.0755]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2584]],\n",
      "\n",
      "         [[ 0.2512]],\n",
      "\n",
      "         [[ 0.1481]],\n",
      "\n",
      "         [[-0.1137]],\n",
      "\n",
      "         [[-0.4525]],\n",
      "\n",
      "         [[ 0.0705]],\n",
      "\n",
      "         [[-0.4145]],\n",
      "\n",
      "         [[-0.4204]],\n",
      "\n",
      "         [[ 0.1042]],\n",
      "\n",
      "         [[ 0.3605]],\n",
      "\n",
      "         [[-0.3377]],\n",
      "\n",
      "         [[ 0.1695]],\n",
      "\n",
      "         [[ 0.1933]],\n",
      "\n",
      "         [[ 0.1696]],\n",
      "\n",
      "         [[-0.2423]],\n",
      "\n",
      "         [[ 0.2901]]],\n",
      "\n",
      "\n",
      "        [[[-0.3400]],\n",
      "\n",
      "         [[-0.0250]],\n",
      "\n",
      "         [[-0.0430]],\n",
      "\n",
      "         [[ 0.4665]],\n",
      "\n",
      "         [[-0.3935]],\n",
      "\n",
      "         [[ 0.2595]],\n",
      "\n",
      "         [[ 0.3279]],\n",
      "\n",
      "         [[ 0.3775]],\n",
      "\n",
      "         [[ 0.3731]],\n",
      "\n",
      "         [[-0.1535]],\n",
      "\n",
      "         [[-0.4104]],\n",
      "\n",
      "         [[-0.0290]],\n",
      "\n",
      "         [[-0.2465]],\n",
      "\n",
      "         [[-0.0590]],\n",
      "\n",
      "         [[ 0.4034]],\n",
      "\n",
      "         [[-0.0119]]],\n",
      "\n",
      "\n",
      "        [[[-0.2175]],\n",
      "\n",
      "         [[-0.0928]],\n",
      "\n",
      "         [[ 0.3438]],\n",
      "\n",
      "         [[ 0.3094]],\n",
      "\n",
      "         [[-0.4090]],\n",
      "\n",
      "         [[-0.3477]],\n",
      "\n",
      "         [[-0.0571]],\n",
      "\n",
      "         [[ 0.1344]],\n",
      "\n",
      "         [[ 0.2892]],\n",
      "\n",
      "         [[-0.0915]],\n",
      "\n",
      "         [[-0.4535]],\n",
      "\n",
      "         [[ 0.3364]],\n",
      "\n",
      "         [[ 0.2721]],\n",
      "\n",
      "         [[ 0.4530]],\n",
      "\n",
      "         [[ 0.3353]],\n",
      "\n",
      "         [[-0.4298]]]], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-0.2878,  0.2241,  0.1634, -0.1354, -0.4685, -0.2129, -0.2544, -0.4002],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([[-0.0948,  0.0240,  0.0344,  ..., -0.0131,  0.0304,  0.0330],\n",
      "        [-0.1013,  0.0625,  0.0350,  ...,  0.0331,  0.0191,  0.0432],\n",
      "        [ 0.0320, -0.0519, -0.0154,  ..., -0.0363,  0.0742, -0.0520],\n",
      "        ...,\n",
      "        [-0.0493, -0.0056,  0.0627,  ...,  0.0019,  0.0136,  0.0371],\n",
      "        [ 0.0310,  0.0456, -0.0135,  ..., -0.0156, -0.0290,  0.0266],\n",
      "        [-0.0433,  0.0087, -0.0515,  ...,  0.0394, -0.0098,  0.0056]],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 5.0529e-02,  5.6514e-02, -3.7805e-02,  4.5309e-02, -5.1458e-02,\n",
      "        -5.6530e-02,  1.4972e-02,  5.7540e-02, -1.6335e-03, -4.4600e-03,\n",
      "        -6.1007e-02, -4.0307e-02, -1.2075e-02, -4.5732e-02,  2.6567e-02,\n",
      "         4.0334e-02, -3.1661e-02, -3.8543e-02,  6.2248e-03, -6.2858e-02,\n",
      "        -4.1062e-02,  1.8872e-02, -2.0258e-02, -2.8373e-02,  7.4110e-05,\n",
      "        -1.2054e-02, -7.1552e-02,  7.8865e-02, -4.7815e-02, -4.4722e-02,\n",
      "         7.3571e-03, -6.3208e-02,  1.0818e-02,  7.4550e-02,  2.3658e-02,\n",
      "        -1.4473e-02, -1.8150e-02, -2.4445e-02,  7.1248e-02,  5.9138e-02,\n",
      "        -7.2144e-02,  2.1098e-02,  4.1290e-02,  8.0056e-02,  1.7023e-02,\n",
      "        -9.2514e-03, -1.3725e-02,  6.1075e-02,  2.0033e-02,  4.3968e-02,\n",
      "        -1.3427e-02, -3.7471e-02,  1.9792e-02, -4.0402e-02,  2.9323e-02,\n",
      "        -5.2083e-02, -2.4858e-02,  3.2060e-02,  5.9218e-02,  5.9784e-02,\n",
      "        -2.7988e-02,  7.3136e-03,  2.0206e-02, -5.2189e-02, -6.4054e-02,\n",
      "        -4.3424e-02, -2.7926e-02, -2.1075e-02,  1.4109e-02, -7.7455e-02,\n",
      "        -5.9726e-02,  5.4634e-02,  7.1979e-02,  2.4944e-03, -2.3153e-02,\n",
      "        -5.3531e-02, -5.2275e-02, -2.4406e-02,  2.4328e-02, -3.3211e-02,\n",
      "         7.4966e-02,  3.2569e-04,  2.6633e-02,  6.8341e-02,  8.2020e-02,\n",
      "         4.0327e-02,  1.4776e-02, -1.6282e-02, -9.6067e-03, -1.7413e-02,\n",
      "         3.4320e-02, -4.2954e-02,  1.8851e-02,  2.8134e-02, -3.8021e-02,\n",
      "         3.8608e-02, -4.8695e-03, -5.9266e-02,  3.0260e-02,  3.6452e-03,\n",
      "         1.2618e-02,  4.8417e-02, -2.8699e-02,  2.6689e-03,  4.2824e-02,\n",
      "        -4.8485e-02, -3.6915e-02, -7.2216e-02,  1.5680e-02,  3.5757e-02,\n",
      "         1.1492e-02, -3.9109e-02,  3.0370e-03,  1.4360e-02,  4.7826e-02,\n",
      "        -1.0679e-02, -3.3140e-02, -6.3968e-03,  2.0236e-03, -5.8128e-02,\n",
      "        -7.0745e-02, -4.6767e-02, -5.9929e-02,  7.6053e-02, -1.7380e-02,\n",
      "         1.0173e-03,  3.0169e-02, -4.7352e-03, -6.9086e-02, -1.7433e-02,\n",
      "         3.4615e-02,  3.1451e-02, -3.0611e-03,  3.2522e-02,  3.8687e-02,\n",
      "        -2.9340e-02,  3.6823e-02, -3.9591e-02,  5.3335e-02,  1.1359e-02,\n",
      "         4.1167e-02,  1.2863e-02,  1.4633e-02,  5.7864e-02, -2.4175e-02,\n",
      "         6.2337e-02,  2.6654e-02,  3.8966e-02,  1.6138e-04,  9.0307e-03,\n",
      "         8.7305e-03,  7.3760e-02, -4.3994e-02, -9.3894e-03, -1.2996e-02,\n",
      "        -4.6506e-02, -6.8573e-02, -3.1540e-02,  1.8012e-02, -3.4041e-02,\n",
      "        -3.6916e-02, -6.8771e-02, -4.7772e-02,  1.7984e-03,  1.8768e-02,\n",
      "         5.5751e-02,  4.9375e-02, -8.8563e-03,  5.9887e-03,  5.4492e-03,\n",
      "         2.8134e-02, -6.2311e-02,  5.6973e-02,  7.3004e-02, -4.6744e-02,\n",
      "         2.7066e-02, -2.9990e-02, -1.7695e-02, -3.9043e-02, -4.4327e-02,\n",
      "        -7.7222e-02, -2.9531e-02, -9.0202e-04, -1.3569e-02,  6.3195e-02,\n",
      "        -6.2658e-02,  2.8313e-02, -2.8518e-02,  3.9436e-02,  4.4823e-02,\n",
      "        -2.4038e-02, -5.0484e-02,  3.4352e-02, -8.1185e-02, -3.1570e-02,\n",
      "         7.9800e-02, -1.0837e-02,  4.1670e-02, -2.8664e-02, -4.7513e-02,\n",
      "        -3.5437e-02,  1.0414e-02, -4.8412e-02, -3.4129e-02,  1.0313e-02,\n",
      "         4.1582e-02,  4.8057e-02,  7.3373e-02,  1.3421e-02, -1.3010e-02,\n",
      "        -4.2146e-02,  4.2571e-02, -4.2655e-02,  4.5421e-02, -8.0260e-02,\n",
      "        -4.9901e-02, -3.5880e-02,  2.7528e-02,  5.4265e-02,  2.4759e-02,\n",
      "        -7.6228e-02,  1.5928e-02, -2.3328e-02, -7.1479e-02,  3.1993e-02,\n",
      "        -2.0113e-02, -3.9053e-02, -3.9582e-02, -4.2809e-02,  6.9142e-02,\n",
      "        -6.5050e-02,  7.6378e-03,  5.1643e-02,  2.4363e-02, -2.4248e-02,\n",
      "         4.9781e-02,  3.8911e-02, -4.7112e-02, -4.5079e-02, -4.3256e-02,\n",
      "         3.6265e-02,  1.4586e-02,  3.5219e-02,  6.3184e-02,  9.9992e-03,\n",
      "        -1.3337e-02,  4.9974e-03, -3.6106e-02,  3.9995e-02, -1.7883e-02,\n",
      "        -4.5557e-02,  6.8079e-02,  7.9840e-03, -1.7625e-02, -7.5351e-02,\n",
      "         2.3388e-03, -2.2476e-02,  3.7830e-02,  3.6278e-03,  4.0691e-02,\n",
      "         4.7784e-02,  4.1712e-03,  3.1892e-02, -1.9449e-02,  7.6918e-02,\n",
      "         4.2737e-03,  5.4993e-03, -2.1376e-02, -8.9915e-03, -2.7540e-02,\n",
      "         1.1046e-02, -4.0193e-03, -5.9103e-02,  4.0149e-02, -2.2512e-02,\n",
      "         4.3629e-02, -5.2097e-03,  2.5558e-02,  1.1182e-02, -4.0095e-02,\n",
      "         3.0008e-02,  3.2860e-02,  1.7027e-02, -6.5199e-02, -4.0385e-02,\n",
      "        -5.9034e-03,  5.0390e-03,  2.6497e-02,  2.8766e-02, -3.0991e-02,\n",
      "        -7.2188e-02, -9.2643e-03,  4.2793e-02, -4.6859e-02,  2.8904e-02,\n",
      "         6.6980e-02,  4.2040e-02,  4.5327e-02,  2.7735e-02, -1.2311e-02,\n",
      "         4.4731e-02,  6.4080e-02,  5.0163e-02,  3.9712e-02, -3.2942e-02,\n",
      "        -1.0057e-02,  3.0766e-02,  2.7916e-02,  4.2690e-02, -7.9736e-02,\n",
      "         4.9866e-02, -2.8740e-02,  6.4906e-02, -7.0528e-02, -2.6097e-02,\n",
      "        -3.3707e-02, -7.2752e-02, -1.9557e-02, -4.0512e-02,  3.5232e-02,\n",
      "        -2.2000e-02,  4.8851e-02, -2.6159e-02, -7.6877e-03,  4.1065e-02,\n",
      "         3.9016e-02,  4.6980e-02,  3.2741e-02,  4.9128e-02,  1.0171e-02,\n",
      "        -6.9971e-03,  2.9285e-02, -4.4354e-02,  6.7803e-02,  5.0439e-02,\n",
      "         3.5516e-02, -1.5379e-02, -5.6884e-02,  3.0670e-03, -2.0119e-02,\n",
      "        -7.7762e-02,  2.6664e-02,  3.6878e-02, -1.8602e-02, -7.1211e-02,\n",
      "         1.2649e-02, -6.5088e-02, -5.0030e-02, -2.5276e-02,  4.7771e-02,\n",
      "        -5.8522e-02,  3.0989e-02, -5.7511e-02, -1.5094e-02,  4.5794e-02,\n",
      "         1.6598e-02,  2.5011e-02,  4.0635e-02,  1.2694e-02, -1.7649e-02,\n",
      "        -5.8344e-02, -4.2214e-02,  2.7205e-02,  3.0539e-02,  2.2967e-02,\n",
      "        -6.3905e-02, -7.7891e-03,  1.1635e-02, -3.0328e-02,  1.8626e-02,\n",
      "        -1.0597e-02, -1.8314e-02,  2.8736e-02,  5.9099e-03, -2.7234e-02,\n",
      "         5.4736e-02, -2.4476e-02, -3.1679e-02,  3.8594e-02, -4.3304e-02,\n",
      "        -7.0828e-02,  3.4757e-02, -3.3184e-02, -7.5082e-02,  1.4552e-02,\n",
      "        -9.1563e-03, -2.7571e-02,  5.7029e-02,  5.1687e-02, -1.8897e-02,\n",
      "         2.9949e-02, -4.1401e-02,  2.0545e-02,  8.1645e-03,  2.0713e-02,\n",
      "         1.2523e-02,  2.4374e-03,  2.9689e-02,  5.3149e-02,  6.4993e-02,\n",
      "        -7.9895e-02, -5.7581e-02,  7.6888e-02,  2.8624e-02, -6.5296e-02,\n",
      "         2.6049e-02, -1.9543e-02, -2.4753e-02,  5.2574e-02, -7.4469e-02,\n",
      "         1.5209e-02,  3.3570e-02, -5.3818e-02,  6.1376e-02, -5.2116e-02,\n",
      "        -7.2075e-02,  7.3966e-02, -5.2817e-02,  1.0588e-02,  2.8080e-02,\n",
      "        -2.0553e-02,  2.7565e-02, -4.9043e-02,  3.3389e-03, -3.9222e-02,\n",
      "        -5.4091e-02, -2.9586e-02,  1.4846e-02,  6.6855e-02, -2.3295e-02,\n",
      "        -4.9629e-03, -1.1861e-02,  2.2097e-02, -7.6229e-02, -4.3277e-02,\n",
      "         9.7304e-03,  7.6900e-02,  3.8737e-02,  1.3185e-02, -1.7886e-02,\n",
      "         1.7483e-02,  5.7085e-02,  2.3738e-02,  2.4463e-02,  8.4747e-03,\n",
      "        -4.0985e-02,  3.3616e-02, -4.4859e-02, -3.9595e-02, -1.4708e-02,\n",
      "        -3.3281e-02, -2.1827e-02,  2.3325e-03, -3.4863e-02,  5.0948e-02,\n",
      "        -5.4037e-02,  2.6310e-02, -3.1919e-02,  2.5189e-02,  4.8621e-02,\n",
      "         4.4171e-02, -5.4063e-02,  1.7385e-02, -4.7295e-03, -7.6882e-03,\n",
      "         4.5826e-02, -1.9468e-02,  2.8763e-02, -6.5320e-02, -3.3914e-02,\n",
      "         3.5573e-02, -5.0281e-02,  5.4649e-02, -2.2187e-02,  3.6940e-02,\n",
      "         2.8914e-02, -7.0218e-02,  6.8412e-02, -3.8196e-03, -4.5710e-03,\n",
      "        -6.7632e-02, -3.6110e-02, -3.1808e-02,  1.9276e-03, -7.5407e-02,\n",
      "        -7.4182e-03,  3.0379e-02, -8.1535e-02,  1.5421e-02, -5.0361e-02,\n",
      "         5.3563e-02,  5.7282e-02,  5.2544e-02,  1.8874e-02,  4.9049e-02,\n",
      "        -2.8639e-02, -3.1784e-02,  2.4839e-02, -7.7430e-03, -4.7992e-02,\n",
      "         3.8138e-02, -4.0018e-02,  1.6216e-02,  5.2579e-02,  2.5679e-02,\n",
      "         3.1805e-02, -2.7578e-02,  3.6099e-02, -9.0556e-03, -1.4825e-02,\n",
      "        -1.7272e-02,  1.6930e-02], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([[-0.0247, -0.0334,  0.0318,  ..., -0.0882, -0.0303,  0.0276],\n",
      "        [ 0.0788,  0.0575, -0.0258,  ...,  0.0176, -0.0623,  0.0664]],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([0.0298, 0.0476], device='cuda:0', requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "#模型的層數\n",
    "num_layers = len(list(bobs_model.parameters()))\n",
    "#模型權重平均運算\n",
    "new_list = list(map(lambda x,y: x + y, list(bobs_model.parameters()), list(bobs_model.parameters())))\n",
    "p = list(model.parameters())\n",
    "print(list(model.parameters()))\n",
    "print(123)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(num_layers):\n",
    "        p[i].set_(new_list[i])\n",
    "\n",
    "print(list(model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 7, 9]\n"
     ]
    }
   ],
   "source": [
    "a = [1, 2, 3]\n",
    "b = [4,5,6]\n",
    "new_list = map(lambda x,y: x + y, a,b)   \n",
    "print(list(new_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
